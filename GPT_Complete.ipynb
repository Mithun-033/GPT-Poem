{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Pretrained Transformer (GPT)\n",
    "In this Project , I made a Decoder only GPT(11.22M params) and trained it on roughly ~4 Mil tokens of data for about ~7 hours on a T4 GPU (16gb VRAM), you can find the hyperparameters used a couple of cells below, I will be going through each code cell and explaining, why I did what I did,and documenting some of the ideas and implementation methods that I found Interesting.\n",
    "\n",
    "The model operates in an autoregressive manner, using masked self-attention to ensure that each token prediction depends only on previously generated tokens. Positional information is added to the token embeddings, though I originally wanted to use modern techniques like ALiBi or RoPE for positional embeddings, A simple linear layer gave satisfactory results for a ~12M param model\n",
    "\n",
    "*Teacher forcing* method was used as the training process, making the model predict one token at a time, this particular model does not have\n",
    "special tokens like [BOS],[EOS],[SEP] as I meant it for to be a infinitely generating.\n",
    "\n",
    "![alt text](transformers-dark.webp)\n",
    "\n",
    "*HuggingFace.co*\n",
    "\n",
    "The above architecture was implemented in the code. However, the encoder block was omitted, as they are generally not used for generative Transformers, along with a few modifications that will be discussed later.\n",
    "\n",
    "The model was also later finetuned on ~30k tokens of poem summary, this was done to teach the model a grammatical structure it needs to follow, while also giving tokens the model is familar with (Poem summary tend to have similar vocabulary to the poems themselves).\n",
    "\n",
    "##### *References* :-\n",
    "*Attention is all you need (2017)*\n",
    "\n",
    "*Language Models are Unsupervised Multitask Learners(2019)*\n",
    "\n",
    "*Andrej Karpthay - Youtube*\n",
    "\n",
    "*Stanford CME295 Transformers & LLMs - Youtube*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported tokenizer and tokenizer trainer libraries to train my own tokenizer, initally I used Whitespace pre-tokenizer, where the words are initally broken into words, but that resulted in sub-optimal generation leading to a lot of broken words and it did not handle unkown tokens well, hence I switched to ByteLevel pre-tokenizer to prevent OOV (Out Of Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:22:13.137005Z",
     "iopub.status.busy": "2026-01-31T04:22:13.136296Z",
     "iopub.status.idle": "2026-01-31T04:22:19.688390Z",
     "shell.execute_reply": "2026-01-31T04:22:19.687785Z",
     "shell.execute_reply.started": "2026-01-31T04:22:13.136968Z"
    },
    "id": "wcNemdfctU_w",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T04:22:22.066885Z",
     "iopub.status.busy": "2026-01-31T04:22:22.065984Z",
     "iopub.status.idle": "2026-01-31T04:22:22.317596Z",
     "shell.execute_reply": "2026-01-31T04:22:22.316912Z",
     "shell.execute_reply.started": "2026-01-31T04:22:22.066852Z"
    },
    "id": "yhSau1s8ZfNA",
    "outputId": "ee84ad79-005d-45bf-95e9-d93558628e37",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the kaggle dataset and unzipping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T04:22:23.849709Z",
     "iopub.status.busy": "2026-01-31T04:22:23.849019Z",
     "iopub.status.idle": "2026-01-31T04:22:24.809291Z",
     "shell.execute_reply": "2026-01-31T04:22:24.808610Z",
     "shell.execute_reply.started": "2026-01-31T04:22:23.849680Z"
    },
    "id": "UiHWwhB0togE",
    "outputId": "e3435dd8-4aca-4f05-ca45-775d7f2db152",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 10.3M  100 10.3M    0     0  15.2M      0 --:--:-- --:--:-- --:--:-- 33.0M\n",
      "Archive:  poem-dataset.zip\n",
      "  inflating: Poems_Dataset.csv       \n",
      "  inflating: poemDatasetWithSummary.csv  \n"
     ]
    }
   ],
   "source": [
    "!curl -L -o poem-dataset.zip \\\n",
    "https://www.kaggle.com/api/v1/datasets/download/marufchowdhury/poem-dataset\n",
    "\n",
    "!unzip poem-dataset.zip\n",
    "\n",
    "df=pd.read_csv(\"Poems_Dataset.csv\")\n",
    "\n",
    "df=df[\"Poem Content\"]\n",
    "\n",
    "data=df.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyper paramaters:-\n",
    "\n",
    "After the model inference , the vocab size seemed to be the bottle neck of the model\n",
    "\n",
    "Here you can see I made the head_size of the model(the dimension of key,query,value) n_embed/n_head, the reason for that will be explained later.\n",
    "\n",
    "I also made the finetuning lr rate much much lower than the lr rate for training , to make sure I dont destroy the poem structure the model is supposed to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T12:56:22.748380Z",
     "iopub.status.busy": "2026-01-31T12:56:22.747750Z",
     "iopub.status.idle": "2026-01-31T12:56:22.752242Z",
     "shell.execute_reply": "2026-01-31T12:56:22.751570Z",
     "shell.execute_reply.started": "2026-01-31T12:56:22.748352Z"
    },
    "id": "vRU-IZWrugfs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "context_window_length=128\n",
    "batch_size=256\n",
    "n_embed=288\n",
    "n_head=9\n",
    "n_layers=8\n",
    "v_size=9000\n",
    "head_size=n_embed//n_head\n",
    "lr_t=0.00007\n",
    "lr_ft=lr_t*0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I made the mistake of training the tokenizer on the entire dataset instead of jus the training set, but I figured it won't make much of a difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:22:55.731865Z",
     "iopub.status.busy": "2026-01-31T04:22:55.731161Z",
     "iopub.status.idle": "2026-01-31T04:23:00.542766Z",
     "shell.execute_reply": "2026-01-31T04:23:00.540185Z",
     "shell.execute_reply.started": "2026-01-31T04:22:55.731835Z"
    },
    "id": "2L-z8OqVmac5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer=ByteLevel(add_prefix_space=True)\n",
    "trainer=BpeTrainer(vocab_size=v_size)\n",
    "tokenizer.train_from_iterator(data,trainer)\n",
    "tokenizer.save(\"Tokenizor.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:03.386290Z",
     "iopub.status.busy": "2026-01-31T04:23:03.385439Z",
     "iopub.status.idle": "2026-01-31T04:23:03.393956Z",
     "shell.execute_reply": "2026-01-31T04:23:03.393326Z",
     "shell.execute_reply.started": "2026-01-31T04:23:03.386258Z"
    },
    "id": "uilE4AdCoxN7",
    "outputId": "264f84cb-801f-4898-fe83-0da3ff6ece0d",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Ġjungle', 'Ġwind', 'Ġwood', 'Ġocean'], [8086, 559, 1099, 1709])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=tokenizer.encode(\"jungle wind wood ocean\")\n",
    "out.tokens,out.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the tokens of the entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:08.026142Z",
     "iopub.status.busy": "2026-01-31T04:23:08.025830Z",
     "iopub.status.idle": "2026-01-31T04:23:20.879947Z",
     "shell.execute_reply": "2026-01-31T04:23:20.879341Z",
     "shell.execute_reply.started": "2026-01-31T04:23:08.026116Z"
    },
    "id": "J9gzukuMDzFG",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5462341"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids=[]\n",
    "\n",
    "for s in data:\n",
    "    all_ids.extend(tokenizer.encode(s).ids)\n",
    "\n",
    "idss=torch.tensor(all_ids,dtype=torch.long).to(device)\n",
    "len(idss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training and validation tokens, though idk why i didnt use train_test_split library, would hve been much cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:29.761440Z",
     "iopub.status.busy": "2026-01-31T04:23:29.761143Z",
     "iopub.status.idle": "2026-01-31T04:23:29.780938Z",
     "shell.execute_reply": "2026-01-31T04:23:29.780292Z",
     "shell.execute_reply.started": "2026-01-31T04:23:29.761412Z"
    },
    "id": "SCPKXJu9E7_4",
    "outputId": "96a7dfb2-35c1-4e36-beeb-5cd50cc4f6ac",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 5442341)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full=idss[:5462341-20000]\n",
    "val_ids=idss[-20000:-1]\n",
    "len(val_ids),len(full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made a generator to yield batches of x and y in Pytorch Tensors with a fixed size (context_window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:32.566730Z",
     "iopub.status.busy": "2026-01-31T04:23:32.565986Z",
     "iopub.status.idle": "2026-01-31T04:23:32.571588Z",
     "shell.execute_reply": "2026-01-31T04:23:32.571013Z",
     "shell.execute_reply.started": "2026-01-31T04:23:32.566698Z"
    },
    "id": "907QdFNEuzYO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generator(ids,batch_size,cwl):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    count=0\n",
    "\n",
    "    for i in range(len(ids)-cwl):\n",
    "        X.append(ids[i:i+cwl])\n",
    "        Y.append(ids[i+1:i+cwl+1])\n",
    "        count+=1\n",
    "\n",
    "        if count==batch_size:\n",
    "            yield torch.stack(X).to(device),torch.stack(Y).to(device)\n",
    "            X=[]\n",
    "            Y=[]\n",
    "            count=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made a class for a single attention head, and later made a class to group multiple heads together using torch.nn.ModuleList\n",
    "\n",
    "Here , to implement masked attention , where the tokens can only \"see\" the tokens preceding them , I used an interesting implementation \n",
    "from Andrej Karpathy who did this in his NanoGPT repo, this was also done in the GPT-2 PyTorch version you can find on huggingface github\n",
    "\n",
    "The idea is to basically create a lower triangular matrix with dimensions (BxTxT), where T represent total token length or in this case the context window length, and multiply it with the weights.\n",
    "\n",
    "#### Why TxT??\n",
    "\n",
    "We get TxT because in the attention formula, we initally calculate the dot product of Keys and Values of \"Every token\" with each other.\n",
    "Here keys represent the learnable features(n_features=head_size) that each token \"represent\". Basically key is a \"identity of a token\"\n",
    "\n",
    "While a query is the \"set of features \" each token best matches with/ searches for.So higher the value of the dot product of key and query, higher is the alignment in the learned feature space, which implies they share more \"context\"\n",
    "\n",
    "So the BxTxT matrix can be thought of a as a lookup table of context logits of all tokens w.r.t each other\n",
    "\n",
    "But the problem here is , the weight matrix we got is not masked attention yet, rather it is biderctional attention which is commonly used in encoder only models like BERT for sentiment analysis, etc.\n",
    "Hence e apply a lower-triangular causal mask to the attention logits, setting all future positions to −∞ before the softmax.\n",
    "\n",
    "Now this could be done in several ways, using jus python implementation, but vectorizing the operations in PyTorch saves a lot of computational time.\n",
    "\n",
    "And finally, we matrix-multiply the softmax-normalized attention weights with the value vectors to obtain a weighted sum of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:35.674260Z",
     "iopub.status.busy": "2026-01-31T04:23:35.673959Z",
     "iopub.status.idle": "2026-01-31T04:23:35.680337Z",
     "shell.execute_reply": "2026-01-31T04:23:35.679772Z",
     "shell.execute_reply.started": "2026-01-31T04:23:35.674233Z"
    },
    "id": "YI8G4WR2rku9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_embed,head_size,bias=False) #(B,T,C)-->(B,T,H)\n",
    "        self.query=nn.Linear(n_embed,head_size,bias=False) #(B,T,C)-->(B,T,H)\n",
    "        self.value=nn.Linear(n_embed,head_size,bias=False)  #(B,T,C)-->(B,T,H\n",
    "        #self.dropout=nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        k=self.key(x)     #(B,T,H)\n",
    "        q=self.query(x)   #(B,T,H)\n",
    "        v=self.value(x)   #(B,T,H)\n",
    "\n",
    "        # Do Dot product of k and q\n",
    "\n",
    "        weights=k@q.transpose(-2,-1)*head_size**-0.5  # (B,T,H) x (B,H,T) --> (B,T,T)\n",
    "        T=x.size(1)\n",
    "        mask=torch.tril(torch.ones(T,T,device=x.device))\n",
    "        weights=weights.masked_fill(mask==0,float('-inf'))\n",
    "        weights=nn.functional.softmax(weights,dim=-1)\n",
    "        #weights = self.dropout(weights)\n",
    "\n",
    "        output=weights@v #(B,T,T) x (B,T,H) --> (B,T,H)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see I just used a library module to have multiple heads , one important feature here is , I specifically made the product of no.of heads and the Head_size to be equal to the embedding dimension to save compute of having a linear projection layer, this was also done in the NanoGPT repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:38.402319Z",
     "iopub.status.busy": "2026-01-31T04:23:38.401815Z",
     "iopub.status.idle": "2026-01-31T04:23:38.407011Z",
     "shell.execute_reply": "2026-01-31T04:23:38.406395Z",
     "shell.execute_reply.started": "2026-01-31T04:23:38.402293Z"
    },
    "id": "W_ekLv4AxyUy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,n_head,head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([AttentionHead(head_size) for _ in range(n_head)])\n",
    "        #self.project=nn.Linear(n_head*head_size,n_embed)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)  # (B,T,H*N)\n",
    "        #out=self.project(out)  # (B,T,H*N) --> (B,T,C) \n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the feedforward layer where a lot of the \"thinking\" of the block happens, In the *Attention is all you need* paper they used 4*n_embed for the output and input of the first and second layer respectively, but to save compute I used 3x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:32:04.342484Z",
     "iopub.status.busy": "2026-01-31T04:32:04.342188Z",
     "iopub.status.idle": "2026-01-31T04:32:04.347353Z",
     "shell.execute_reply": "2026-01-31T04:32:04.346746Z",
     "shell.execute_reply.started": "2026-01-31T04:32:04.342455Z"
    },
    "id": "0SkDUQSkzaDf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.FF=nn.Sequential(\n",
    "            nn.Linear(n_embed,3*n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(3*n_embed,n_embed),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.FF(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important change from the actual GPT architecture was the use of Pre-LayerNorm instead of Post-LayerNorm, which was discussed in the *Language Models are Unsupervised Multitask Learners (2019)* paper.\n",
    "\n",
    "You can also see , We are not getting the output of the model, rather we are incrementing the input by the output of the model after the feedforward, this is to show that each block of a transformer does not give their own fresh interpertation of the data, rather they add to the information and bending the vectors to make meaningful data. This method makes the model \"reason\" well with depth of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:42.832943Z",
     "iopub.status.busy": "2026-01-31T04:23:42.832648Z",
     "iopub.status.idle": "2026-01-31T04:23:42.838054Z",
     "shell.execute_reply": "2026-01-31T04:23:42.837344Z",
     "shell.execute_reply.started": "2026-01-31T04:23:42.832921Z"
    },
    "id": "tvYtXEmo05oI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embed,n_head):\n",
    "        super().__init__()\n",
    "        head_size=n_embed//n_head\n",
    "        self.SelfAtt = MultiHead(n_head, head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x + self.SelfAtt(self.ln1(x)) \n",
    "        x=x + self.ffwd(self.ln2(x))\n",
    "        return x  #(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final GPT class where I call all the previous class objects to make the decoder architecture, I also add the positional embeddings here.\n",
    "\n",
    "Another change from the original GPT architecture is the inclusion of one final Layernorm after all the blocks before the Linear projection of embeddings to the vocab.\n",
    "\n",
    "In the generate function , I get the input tokens and only take the last 128 tokens to pass as the input to the model, since my model only has a static context window length, in this case 128 tokens\n",
    "\n",
    "THE MOST IMPORTANT CHANGE that I did to my model which made the Greatest impact was to add Temperature.\n",
    "Temperature is basically a scaling factor which flattens/increases the peak of the token distrubution of the output logits, this makes the model more \"creative\" when generating text.\n",
    "\n",
    "Though in my case it made my model speak less and less broken words when I reduced the temperature value. \n",
    "For example,\n",
    "\n",
    "\n",
    "### Temperature = 1:\n",
    "\n",
    "Enter Prompt / Initial tokens: cool breeze\n",
    "\n",
    "Generated Text:\n",
    "\n",
    "*cool breeze .*  \n",
    "*We walk the rest of the drowned* . \n",
    "*It took the sub stit ution on earth* .  \n",
    "*Be unable up from the hills  circ uit fountain through the P ver , contain the chill .  The new - black continent , s ales are walking* *holes  them inside the shade of May ; we hear*\n",
    "*The meal simply lod ges like a horse*\n",
    "*that sw irls Fl ots ,  Time of ir ast the dark - bl ades .*\n",
    "*So ings d ent ird to its nest in ils ,  *\n",
    "*As the read est leaf of clouds ,*\n",
    "*A ng en she ind ulating the road to the dis he ast ,  Pl acks*\n",
    "\n",
    "Here you can see , after the inital 15-20 tokens, the model begins to speak broken sub words.This gets worse as the we come to the end of the generation. This word breaking was way worse before I fine-tuned the model on english grammer\n",
    "\n",
    "\n",
    "### Temperature = 0.57\n",
    "\n",
    "Enter Prompt / Initial tokens: cool breeze\n",
    "\n",
    "Generated Text: \n",
    "\n",
    "*cool breeze .* \n",
    "*The natural world of my voice .   Or I had a house ,*\n",
    "*and I meant to contain the time  into the walls of the sun ,*\n",
    "*and the bottom of my body ,  our first , and the world ,*\n",
    "*and my mother who had*\n",
    "*a woman*\n",
    "*their long .*\n",
    "*And when she was*\n",
    "*or the way*\n",
    "*of the word was*\n",
    "*to it ,   the coming back*\n",
    "*and the time*\n",
    "*of her hands   that was*\n",
    "*of the way*\n",
    "*the light   the way*\n",
    "*of the same  of the body*\n",
    "\n",
    "Here the we can see there is not a single broken sub word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:23:45.842608Z",
     "iopub.status.busy": "2026-01-31T04:23:45.842276Z",
     "iopub.status.idle": "2026-01-31T04:23:45.885717Z",
     "shell.execute_reply": "2026-01-31T04:23:45.885006Z",
     "shell.execute_reply.started": "2026-01-31T04:23:45.842574Z"
    },
    "id": "fuWPQlPN21Pf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(v_size,n_embed)  # (B,T) --> (B,T,C)\n",
    "        self.pos_embed=nn.Embedding(context_window_length,n_embed) # (T) --> (T,C)\n",
    "\n",
    "        self.blocks=nn.Sequential(*[Block(n_embed,n_head) for _ in range(n_layers)])\n",
    "        self.final_layernorm = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, v_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x ==> (B,T)\n",
    "\n",
    "        tok_embeds=self.embed(x) # (B,T,C)\n",
    "        pos_embeds=self.pos_embed(torch.arange(x.size(1),device=x.device)) #(T,C)\n",
    "        x=tok_embeds + pos_embeds # pos_embed r broadcasted and added to every batch element\n",
    "\n",
    "        x=self.blocks(x)\n",
    "        x=self.final_layernorm(x)\n",
    "        logits=self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(model,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx.size(1)>context_window_length:\n",
    "                idx_cond=idx[:,-context_window_length:]\n",
    "            else:\n",
    "                idx_cond=idx\n",
    "\n",
    "            logits=model(idx_cond)\n",
    "            probs=torch.softmax(logits[:,-1,:],dim=-1)\n",
    "            next_token=torch.multinomial(probs,1)  #Chossing one token based on the probability distribution\n",
    "            idx=torch.cat((idx,next_token),dim=1) #Adding the new token to the existing sequence\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I did 2 new things other than the normal training process\n",
    "\n",
    "1.torch.compile(), this makes the training faster after the first few batches \n",
    "\n",
    "2.Added a lr schedular to reduce the learning rate after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T04:32:12.669217Z",
     "iopub.status.busy": "2026-01-31T04:32:12.668649Z",
     "iopub.status.idle": "2026-01-31T04:32:12.778803Z",
     "shell.execute_reply": "2026-01-31T04:32:12.778253Z",
     "shell.execute_reply.started": "2026-01-31T04:32:12.669191Z"
    },
    "id": "M__qjCFr6hpt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model=GPT().to(device)\n",
    "model=torch.compile(model)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=lr_t,fused=True)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "epochs=20\n",
    "def lr_lambda(epoch):\n",
    "    return 0.5*(1+math.cos(math.pi*epoch/epochs))\n",
    "\n",
    "scheduler=LambdaLR(optimizer,lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T04:32:16.336652Z",
     "iopub.status.busy": "2026-01-31T04:32:16.335993Z",
     "iopub.status.idle": "2026-01-31T04:32:16.369820Z",
     "shell.execute_reply": "2026-01-31T04:32:16.369125Z",
     "shell.execute_reply.started": "2026-01-31T04:32:16.336622Z"
    },
    "id": "yG42afNYBChY",
    "outputId": "fc0410f7-0a63-4ea1-f78a-a8b82fee215d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used Gradscalar and autocast to train the params in FP16 instead of FP32, this reduced the compute by a lot, and also made training on T4 GPU, which I got free access to in google colab and kaggle, much more efficient since T4 are known to have slower compute speed for FP32.\n",
    "\n",
    "The validation loss for 3 epochs were ~5.2, ~5.0, ~4,98. So I stopped training after 3rd epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T06:40:18.238039Z",
     "iopub.status.busy": "2026-01-31T06:40:18.237435Z",
     "iopub.status.idle": "2026-01-31T12:40:36.481730Z",
     "shell.execute_reply": "2026-01-31T12:40:36.480798Z",
     "shell.execute_reply.started": "2026-01-31T06:40:18.238009Z"
    },
    "id": "SMOyq2NC7d6n",
    "outputId": "40e0f6f7-78bd-4870-bd8e-f3e975a8f5d1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler=torch.amp.GradScaler('cuda')\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    step=0\n",
    "    start_epoch=time.time()\n",
    "    last_print_time=start_epoch\n",
    "\n",
    "    for x,y in generator(full,batch_size,context_window_length):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits=model(x)\n",
    "            logits=logits.view(-1,logits.size(-1))\n",
    "            y=y.view(-1)\n",
    "            loss=criterion(logits,y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        step+=1\n",
    "        if step%150==0:\n",
    "            now=time.time()\n",
    "            print(\n",
    "                f\"Epoch: {i+1}, \"\n",
    "                f\"Step: {step}, \"\n",
    "                f\"Loss: {loss.item():.4f}, \"\n",
    "                f\"Time/150 batches: {(now-last_print_time):.2f} sec\")\n",
    "            last_print_time=now\n",
    "        if step%1500==0:\n",
    "            torch.save(model.state_dict(),\"Temp_model.pt\")\n",
    "    scheduler.step()\n",
    "        \n",
    "    \n",
    "    end_epoch=time.time()\n",
    "    print(f\"Epoch {i+1} total time: {(end_epoch-start_epoch):.2f} sec\")\n",
    "    torch.save(model.state_dict(),f\"Mmodel_epoch__{i+1}.pt\")\n",
    "\n",
    "    avg_loss=0\n",
    "    count=0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x,y in generator(val_ids,batch_size,context_window_length\n",
    "        ):\n",
    "            logits=model(x)\n",
    "            logits=logits.view(-1,logits.size(-1))\n",
    "            y=y.view(-1)\n",
    "            loss=criterion(logits,y)\n",
    "            count+=1\n",
    "            avg_loss+=loss.item()\n",
    "\n",
    "    print(f\"Model_{i} Val Loss:{avg_loss/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I decided to fine-tune the model because, I noticed that even though the model spoke complete words it did not have a structure. So i decided to train it on proper english sentence which share similar vocabular, the poem summaries of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:05:52.650216Z",
     "iopub.status.busy": "2026-01-31T13:05:52.649656Z",
     "iopub.status.idle": "2026-01-31T13:05:53.333262Z",
     "shell.execute_reply": "2026-01-31T13:05:53.332677Z",
     "shell.execute_reply.started": "2026-01-31T13:05:52.650187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now to finetune the model to proper english grammer while having a similar vocabular, ill be fine-tuning it on \n",
    "# the poem's summary, jus 10% of the total summary, jus to tweak/guide the model in the direction not completely\n",
    "# change the generation\n",
    "df_ft=pd.read_csv(\"/kaggle/working/poemDatasetWithSummary.csv\")\n",
    "x=df_ft[\"jist\"].tolist()\n",
    "ids_ft=[]\n",
    "for i in x:\n",
    "    ids_ft.extend(tokenizer.encode(i).ids)\n",
    "ids_ft=ids_ft[:30000]\n",
    "ids_ft=torch.tensor(ids_ft,dtype=torch.long).to(device)\n",
    "len(ids_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:06:06.732174Z",
     "iopub.status.busy": "2026-01-31T13:06:06.731869Z",
     "iopub.status.idle": "2026-01-31T13:06:06.990730Z",
     "shell.execute_reply": "2026-01-31T13:06:06.989886Z",
     "shell.execute_reply.started": "2026-01-31T13:06:06.732148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer.from_file(\"Tokenizor.json\")\n",
    "sd=torch.load(\"Mmodel_epoch__3.pt\",map_location=device)\n",
    "sd={k.replace(\"_orig_mod.\",\"\"):v for k,v in sd.items()}\n",
    "model_ft=GPT()\n",
    "model_ft.load_state_dict(sd)\n",
    "model_ft.to(device)\n",
    "model_ft.eval()\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:07:44.106070Z",
     "iopub.status.busy": "2026-01-31T13:07:44.105484Z",
     "iopub.status.idle": "2026-01-31T13:07:44.111102Z",
     "shell.execute_reply": "2026-01-31T13:07:44.110513Z",
     "shell.execute_reply.started": "2026-01-31T13:07:44.106040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer_ft=torch.optim.AdamW(model_ft.parameters(),lr=lr_ft)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:08:54.007763Z",
     "iopub.status.busy": "2026-01-31T13:08:54.006885Z",
     "iopub.status.idle": "2026-01-31T13:13:04.818541Z",
     "shell.execute_reply": "2026-01-31T13:13:04.817850Z",
     "shell.execute_reply.started": "2026-01-31T13:08:54.007733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    step=0\n",
    "    curr=time.time()\n",
    "    final=curr\n",
    "    for x,y in generator(ids_ft,batch_size,context_window_length):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits=model_ft(x)\n",
    "        logits=logits.view(-1,logits.shape[-1])\n",
    "        y=y.view(-1)\n",
    "\n",
    "        loss=criterion(logits,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step+=1\n",
    "\n",
    "        if step%10==0:\n",
    "            final=time.time()\n",
    "            print(\"Loss:\",loss.item(),\"Time:\",final-curr)\n",
    "            curr=final\n",
    "\n",
    "print(\"Finetuning Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:13:12.712105Z",
     "iopub.status.busy": "2026-01-31T13:13:12.711339Z",
     "iopub.status.idle": "2026-01-31T13:13:12.796488Z",
     "shell.execute_reply": "2026-01-31T13:13:12.795905Z",
     "shell.execute_reply.started": "2026-01-31T13:13:12.712073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"Model_FineTuned.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
