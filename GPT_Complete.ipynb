{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2tjYJUz7MFA","outputId":"47a1cf8f-d548-4902-ecda-e8d3cdca882f","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:02.729936Z","iopub.execute_input":"2026-01-31T04:22:02.730215Z","iopub.status.idle":"2026-01-31T04:22:08.331157Z","shell.execute_reply.started":"2026-01-31T04:22:02.730186Z","shell.execute_reply":"2026-01-31T04:22:08.330271Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import ByteLevel\nfrom tokenizers.trainers import BpeTrainer\nimport torch\nimport torch.nn as nn\nfrom torchinfo import summary\nimport time\nimport math\nfrom torch.optim.lr_scheduler import LambdaLR","metadata":{"id":"wcNemdfctU_w","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:13.136296Z","iopub.execute_input":"2026-01-31T04:22:13.137005Z","iopub.status.idle":"2026-01-31T04:22:19.688390Z","shell.execute_reply.started":"2026-01-31T04:22:13.136968Z","shell.execute_reply":"2026-01-31T04:22:19.687785Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"yhSau1s8ZfNA","outputId":"ee84ad79-005d-45bf-95e9-d93558628e37","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:22.065984Z","iopub.execute_input":"2026-01-31T04:22:22.066885Z","iopub.status.idle":"2026-01-31T04:22:22.317596Z","shell.execute_reply.started":"2026-01-31T04:22:22.066852Z","shell.execute_reply":"2026-01-31T04:22:22.316912Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!curl -L -o poem-dataset.zip \\\nhttps://www.kaggle.com/api/v1/datasets/download/marufchowdhury/poem-dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UiHWwhB0togE","outputId":"e3435dd8-4aca-4f05-ca45-775d7f2db152","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:23.849019Z","iopub.execute_input":"2026-01-31T04:22:23.849709Z","iopub.status.idle":"2026-01-31T04:22:24.809291Z","shell.execute_reply.started":"2026-01-31T04:22:23.849680Z","shell.execute_reply":"2026-01-31T04:22:24.808610Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 10.3M  100 10.3M    0     0  15.5M      0 --:--:-- --:--:-- --:--:-- 15.5M\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!unzip poem-dataset.zip\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1boynZ04t9vB","outputId":"87687e7d-5a61-469b-9593-956ecfef7dec","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:27.925742Z","iopub.execute_input":"2026-01-31T04:22:27.926370Z","iopub.status.idle":"2026-01-31T04:22:44.558637Z","shell.execute_reply.started":"2026-01-31T04:22:27.926339Z","shell.execute_reply":"2026-01-31T04:22:44.557857Z"}},"outputs":[{"name":"stdout","text":"Archive:  poem-dataset.zip\nreplace Poems_Dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df=pd.read_csv(\"Poems_Dataset.csv\")\ndf=df[\"Poem Content\"]\ndata=df.tolist()\n","metadata":{"id":"tUbDM-EOuWcY","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:49.201661Z","iopub.execute_input":"2026-01-31T04:22:49.202366Z","iopub.status.idle":"2026-01-31T04:22:49.483515Z","shell.execute_reply.started":"2026-01-31T04:22:49.202332Z","shell.execute_reply":"2026-01-31T04:22:49.482934Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Hyper Parameters\ncontext_window_length=128\nbatch_size=256\nn_embed=288\nn_head=9\nn_layers=8\nv_size=9000\nhead_size=n_embed//n_head\nlr_t=0.00007\nlr_ft=lr_t*0.05","metadata":{"id":"vRU-IZWrugfs","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T12:56:22.747750Z","iopub.execute_input":"2026-01-31T12:56:22.748380Z","iopub.status.idle":"2026-01-31T12:56:22.752242Z","shell.execute_reply.started":"2026-01-31T12:56:22.748352Z","shell.execute_reply":"2026-01-31T12:56:22.751570Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"tokenizer=Tokenizer(BPE())\ntokenizer.pre_tokenizer=ByteLevel(add_prefix_space=True)\ntrainer=BpeTrainer(vocab_size=v_size)\ntokenizer.train_from_iterator(data,trainer)","metadata":{"id":"2L-z8OqVmac5","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:22:55.731161Z","iopub.execute_input":"2026-01-31T04:22:55.731865Z","iopub.status.idle":"2026-01-31T04:23:00.542766Z","shell.execute_reply.started":"2026-01-31T04:22:55.731835Z","shell.execute_reply":"2026-01-31T04:23:00.540185Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer.save(\"Tokenizor.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T12:54:53.966789Z","iopub.execute_input":"2026-01-31T12:54:53.967355Z","iopub.status.idle":"2026-01-31T12:54:53.990054Z","shell.execute_reply.started":"2026-01-31T12:54:53.967327Z","shell.execute_reply":"2026-01-31T12:54:53.989289Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"out=tokenizer.encode(\"hello my guy how are you\")\nout.tokens,out.ids\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uilE4AdCoxN7","outputId":"264f84cb-801f-4898-fe83-0da3ff6ece0d","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:03.385439Z","iopub.execute_input":"2026-01-31T04:23:03.386290Z","iopub.status.idle":"2026-01-31T04:23:03.393956Z","shell.execute_reply.started":"2026-01-31T04:23:03.386258Z","shell.execute_reply":"2026-01-31T04:23:03.393326Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(['Ġhell', 'o', 'Ġmy', 'Ġguy', 'Ġhow', 'Ġare', 'Ġyou'],\n [2368, 78, 280, 5194, 588, 363, 257])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"all_ids=[]\n\nfor s in data:\n    all_ids.extend(tokenizer.encode(s).ids)\n\nidss=torch.tensor(all_ids,dtype=torch.long).to(device)","metadata":{"id":"J9gzukuMDzFG","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:08.025830Z","iopub.execute_input":"2026-01-31T04:23:08.026142Z","iopub.status.idle":"2026-01-31T04:23:20.879947Z","shell.execute_reply.started":"2026-01-31T04:23:08.026116Z","shell.execute_reply":"2026-01-31T04:23:20.879341Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"len(idss)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o27nzmcAco9N","outputId":"7ac849e7-e5e2-4ecb-c7ea-fd093aea6d3b","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:27.599123Z","iopub.execute_input":"2026-01-31T04:23:27.599993Z","iopub.status.idle":"2026-01-31T04:23:27.604814Z","shell.execute_reply.started":"2026-01-31T04:23:27.599962Z","shell.execute_reply":"2026-01-31T04:23:27.604285Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"5462341"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"full=idss[:5462341-20000]\nval_ids=idss[-20000:-1]\nlen(val_ids),len(full)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCPKXJu9E7_4","outputId":"96a7dfb2-35c1-4e36-beeb-5cd50cc4f6ac","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:29.761143Z","iopub.execute_input":"2026-01-31T04:23:29.761440Z","iopub.status.idle":"2026-01-31T04:23:29.780938Z","shell.execute_reply.started":"2026-01-31T04:23:29.761412Z","shell.execute_reply":"2026-01-31T04:23:29.780292Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"400000"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def generator(ids,batch_size,cwl):\n    X=[]\n    Y=[]\n    count=0\n\n    for i in range(len(ids)-cwl):\n        X.append(ids[i:i+cwl])\n        Y.append(ids[i+1:i+cwl+1])\n        count+=1\n\n        if count==batch_size:\n            yield torch.stack(X).to(device),torch.stack(Y).to(device)\n            X=[]\n            Y=[]\n            count=0","metadata":{"id":"907QdFNEuzYO","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:32.565986Z","iopub.execute_input":"2026-01-31T04:23:32.566730Z","iopub.status.idle":"2026-01-31T04:23:32.571588Z","shell.execute_reply.started":"2026-01-31T04:23:32.566698Z","shell.execute_reply":"2026-01-31T04:23:32.571013Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self,head_size):\n        super().__init__()\n        self.key=nn.Linear(n_embed,head_size,bias=False) #(B,T,C)-->(B,T,H)\n        self.query=nn.Linear(n_embed,head_size,bias=False) #(B,T,C)-->(B,T,H)\n        self.value=nn.Linear(n_embed,head_size,bias=False)  #(B,T,C)-->(B,T,H\n        #self.dropout=nn.Dropout(0.2)\n\n    def forward(self,x):\n        k=self.key(x)     #(B,T,H)\n        q=self.query(x)   #(B,T,H)\n        v=self.value(x)   #(B,T,H)\n\n        # Do Dot product of k and q\n\n        weights=k@q.transpose(-2,-1)*head_size**-0.5  # (B,T,H) x (B,H,T) --> (B,T,T)\n        T=x.size(1)\n        mask=torch.tril(torch.ones(T,T,device=x.device))\n        weights=weights.masked_fill(mask==0,float('-inf'))\n        weights=nn.functional.softmax(weights,dim=-1)\n        #weights = self.dropout(weights)\n\n        output=weights@v #(B,T,T) x (B,T,H) --> (B,T,H)\n        return output","metadata":{"id":"YI8G4WR2rku9","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:35.673959Z","iopub.execute_input":"2026-01-31T04:23:35.674260Z","iopub.status.idle":"2026-01-31T04:23:35.680337Z","shell.execute_reply.started":"2026-01-31T04:23:35.674233Z","shell.execute_reply":"2026-01-31T04:23:35.679772Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MultiHead(nn.Module):\n    def __init__(self,n_head,head_size):\n        super().__init__()\n        self.heads=nn.ModuleList([AttentionHead(head_size) for _ in range(n_head)])\n        #self.project=nn.Linear(n_head*head_size,n_embed)\n        self.dropout=nn.Dropout(0.2)\n    def forward(self,x):\n        out=torch.cat([h(x) for h in self.heads],dim=-1)  # (B,T,H*N)\n        #out=self.project(out)  # (B,T,H*N) --> (B,T,C) \n        out = self.dropout(out)\n        return out","metadata":{"id":"W_ekLv4AxyUy","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:38.401815Z","iopub.execute_input":"2026-01-31T04:23:38.402319Z","iopub.status.idle":"2026-01-31T04:23:38.407011Z","shell.execute_reply.started":"2026-01-31T04:23:38.402293Z","shell.execute_reply":"2026-01-31T04:23:38.406395Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.FF=nn.Sequential(\n            nn.Linear(n_embed,3*n_embed),\n            nn.GELU(),\n            nn.Linear(3*n_embed,n_embed),\n            nn.Dropout(0.2)\n        )\n\n    def forward(self,x):\n        return self.FF(x)","metadata":{"id":"0SkDUQSkzaDf","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:32:04.342188Z","iopub.execute_input":"2026-01-31T04:32:04.342484Z","iopub.status.idle":"2026-01-31T04:32:04.347353Z","shell.execute_reply.started":"2026-01-31T04:32:04.342455Z","shell.execute_reply":"2026-01-31T04:32:04.346746Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self,n_embed,n_head):\n        super().__init__()\n        head_size=n_embed//n_head\n        self.SelfAtt = MultiHead(n_head, head_size)\n        self.ffwd = FeedForward()\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n\n    def forward(self,x):\n        x=x + self.SelfAtt(self.ln1(x)) \n        x=x + self.ffwd(self.ln2(x))\n        return x  #(B,T,C)","metadata":{"id":"tvYtXEmo05oI","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:42.832648Z","iopub.execute_input":"2026-01-31T04:23:42.832943Z","iopub.status.idle":"2026-01-31T04:23:42.838054Z","shell.execute_reply.started":"2026-01-31T04:23:42.832921Z","shell.execute_reply":"2026-01-31T04:23:42.837344Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embed=nn.Embedding(v_size,n_embed)  # (B,T) --> (B,T,C)\n        self.pos_embed=nn.Embedding(context_window_length,n_embed) # (T) --> (T,C)\n\n        self.blocks=nn.Sequential(*[Block(n_embed,n_head) for _ in range(n_layers)])\n        self.final_layernorm = nn.LayerNorm(n_embed) # final layer norm\n        self.lm_head = nn.Linear(n_embed, v_size)\n\n    def forward(self,x):\n        # x ==> (B,T)\n\n        tok_embeds=self.embed(x) # (B,T,C)\n        pos_embeds=self.pos_embed(torch.arange(x.size(1),device=x.device)) #(T,C)\n        x=tok_embeds + pos_embeds # pos_embed r broadcasted and added to every batch element\n\n        x=self.blocks(x)\n        x=self.final_layernorm(x)\n        logits=self.lm_head(x)\n\n        return logits\n\n\n    @torch.no_grad()\n    def generate(model,idx,max_new_tokens):\n        for _ in range(max_new_tokens):\n            if idx.size(1)>context_window_length:\n                idx_cond=idx[:,-context_window_length:]\n            else:\n                idx_cond=idx\n\n            logits=model(idx_cond)\n            probs=torch.softmax(logits[:,-1,:],dim=-1)\n            next_token=torch.multinomial(probs,1)\n            idx=torch.cat((idx,next_token),dim=1)\n\n        return idx\n","metadata":{"id":"fuWPQlPN21Pf","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:23:45.842276Z","iopub.execute_input":"2026-01-31T04:23:45.842608Z","iopub.status.idle":"2026-01-31T04:23:45.885717Z","shell.execute_reply.started":"2026-01-31T04:23:45.842574Z","shell.execute_reply":"2026-01-31T04:23:45.885006Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model=GPT().to(device)\nmodel=torch.compile(model)\noptimizer=torch.optim.AdamW(model.parameters(),lr=lr_t,fused=True)\ncriterion=nn.CrossEntropyLoss()\nepochs=20\ndef lr_lambda(epoch):\n    return 0.5*(1+math.cos(math.pi*epoch/epochs))\n\nscheduler=LambdaLR(optimizer,lr_lambda)\n","metadata":{"id":"M__qjCFr6hpt","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:32:12.668649Z","iopub.execute_input":"2026-01-31T04:32:12.669217Z","iopub.status.idle":"2026-01-31T04:32:12.778803Z","shell.execute_reply.started":"2026-01-31T04:32:12.669191Z","shell.execute_reply":"2026-01-31T04:32:12.778253Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"summary(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yG42afNYBChY","outputId":"fc0410f7-0a63-4ea1-f78a-a8b82fee215d","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T04:32:16.335993Z","iopub.execute_input":"2026-01-31T04:32:16.336652Z","iopub.status.idle":"2026-01-31T04:32:16.369820Z","shell.execute_reply.started":"2026-01-31T04:32:16.336622Z","shell.execute_reply":"2026-01-31T04:32:16.369125Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"================================================================================\nLayer (type:depth-idx)                                  Param #\n================================================================================\nOptimizedModule                                         --\n├─GPT: 1-1                                              --\n│    └─Embedding: 2-1                                   2,592,000\n│    └─Embedding: 2-2                                   36,864\n│    └─Sequential: 2-3                                  --\n│    │    └─Block: 3-1                                  748,800\n│    │    └─Block: 3-2                                  748,800\n│    │    └─Block: 3-3                                  748,800\n│    │    └─Block: 3-4                                  748,800\n│    │    └─Block: 3-5                                  748,800\n│    │    └─Block: 3-6                                  748,800\n│    │    └─Block: 3-7                                  748,800\n│    │    └─Block: 3-8                                  748,800\n│    └─LayerNorm: 2-4                                   576\n│    └─Linear: 2-5                                      2,601,000\n================================================================================\nTotal params: 11,220,840\nTrainable params: 11,220,840\nNon-trainable params: 0\n================================================================================"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"scaler=torch.cuda.amp.GradScaler()\n\nfor i in range(epochs):\n    model.train()\n    step=0\n    start_epoch=time.time()\n    last_print_time=start_epoch\n\n    for x,y in generator(full,batch_size,context_window_length):\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.cuda.amp.autocast():\n            logits=model(x)\n            logits=logits.view(-1,logits.size(-1))\n            y=y.view(-1)\n            loss=criterion(logits,y)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        step+=1\n        if step%150==0:\n            now=time.time()\n            print(\n                f\"Epoch: {i+1}, \"\n                f\"Step: {step}, \"\n                f\"Loss: {loss.item():.4f}, \"\n                f\"Time/150 batches: {(now-last_print_time):.2f} sec\")\n            last_print_time=now\n        if step%1500==0:\n            torch.save(model.state_dict(),\"Temp_model.pt\")\n    scheduler.step()\n        \n    \n    end_epoch=time.time()\n    print(f\"Epoch {i+1} total time: {(end_epoch-start_epoch):.2f} sec\")\n    torch.save(model.state_dict(),f\"Mmodel_epoch__{i+1}.pt\")\n\n    avg_loss=0\n    count=0\n    with torch.no_grad():\n        model.eval()\n        for x,y in generator(val_ids,batch_size,context_window_length):\n            logits=model(x)\n            logits=logits.view(-1,logits.size(-1))\n            y=y.view(-1)\n            loss=criterion(logits,y)\n            count+=1\n            avg_loss+=loss.item()\n\n    print(f\"Model_{i} Val Loss:{avg_loss/count}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SMOyq2NC7d6n","outputId":"40e0f6f7-78bd-4870-bd8e-f3e975a8f5d1","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:40:18.237435Z","iopub.execute_input":"2026-01-31T06:40:18.238039Z","iopub.status.idle":"2026-01-31T12:40:36.481730Z","shell.execute_reply.started":"2026-01-31T06:40:18.238009Z","shell.execute_reply":"2026-01-31T12:40:36.480798Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/3561993389.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler=torch.cuda.amp.GradScaler()\n/tmp/ipykernel_55/3561993389.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Step: 150, Loss: 4.3089, Time/100 batches: 51.07 sec\nEpoch: 1, Step: 300, Loss: 4.7733, Time/100 batches: 49.82 sec\nEpoch: 1, Step: 450, Loss: 5.1085, Time/100 batches: 50.89 sec\nEpoch: 1, Step: 600, Loss: 4.6715, Time/100 batches: 50.21 sec\nEpoch: 1, Step: 750, Loss: 5.7537, Time/100 batches: 50.58 sec\nEpoch: 1, Step: 900, Loss: 5.0312, Time/100 batches: 50.91 sec\nEpoch: 1, Step: 1050, Loss: 4.9598, Time/100 batches: 50.92 sec\nEpoch: 1, Step: 1200, Loss: 4.9468, Time/100 batches: 50.20 sec\nEpoch: 1, Step: 1350, Loss: 4.9424, Time/100 batches: 50.21 sec\nEpoch: 1, Step: 1500, Loss: 5.5360, Time/100 batches: 50.58 sec\nEpoch: 1, Step: 1650, Loss: 4.8255, Time/100 batches: 50.87 sec\nEpoch: 1, Step: 1800, Loss: 5.2973, Time/100 batches: 50.73 sec\nEpoch: 1, Step: 1950, Loss: 5.5464, Time/100 batches: 50.82 sec\nEpoch: 1, Step: 2100, Loss: 5.7437, Time/100 batches: 50.91 sec\nEpoch: 1, Step: 2250, Loss: 5.5141, Time/100 batches: 50.71 sec\nEpoch: 1, Step: 2400, Loss: 6.0416, Time/100 batches: 51.06 sec\nEpoch: 1, Step: 2550, Loss: 5.6173, Time/100 batches: 51.07 sec\nEpoch: 1, Step: 2700, Loss: 5.0827, Time/100 batches: 50.24 sec\nEpoch: 1, Step: 2850, Loss: 5.9741, Time/100 batches: 50.25 sec\nEpoch: 1, Step: 3000, Loss: 5.6757, Time/100 batches: 50.30 sec\nEpoch: 1, Step: 3150, Loss: 5.6976, Time/100 batches: 50.35 sec\nEpoch: 1, Step: 3300, Loss: 5.3891, Time/100 batches: 50.51 sec\nEpoch: 1, Step: 3450, Loss: 5.2339, Time/100 batches: 50.94 sec\nEpoch: 1, Step: 3600, Loss: 4.9328, Time/100 batches: 50.32 sec\nEpoch: 1, Step: 3750, Loss: 5.7480, Time/100 batches: 50.30 sec\nEpoch: 1, Step: 3900, Loss: 5.4655, Time/100 batches: 50.21 sec\nEpoch: 1, Step: 4050, Loss: 5.7067, Time/100 batches: 50.51 sec\nEpoch: 1, Step: 4200, Loss: 5.6085, Time/100 batches: 50.82 sec\nEpoch: 1, Step: 4350, Loss: 5.4119, Time/100 batches: 50.67 sec\nEpoch: 1, Step: 4500, Loss: 5.9915, Time/100 batches: 50.32 sec\nEpoch: 1, Step: 4650, Loss: 5.8610, Time/100 batches: 50.36 sec\nEpoch: 1, Step: 4800, Loss: 5.3326, Time/100 batches: 51.00 sec\nEpoch: 1, Step: 4950, Loss: 5.5586, Time/100 batches: 50.87 sec\nEpoch: 1, Step: 5100, Loss: 5.3825, Time/100 batches: 51.06 sec\nEpoch: 1, Step: 5250, Loss: 5.5647, Time/100 batches: 50.44 sec\nEpoch: 1, Step: 5400, Loss: 5.2545, Time/100 batches: 50.26 sec\nEpoch: 1, Step: 5550, Loss: 5.1741, Time/100 batches: 50.37 sec\nEpoch: 1, Step: 5700, Loss: 5.1401, Time/100 batches: 50.77 sec\nEpoch: 1, Step: 5850, Loss: 4.8007, Time/100 batches: 50.64 sec\nEpoch: 1, Step: 6000, Loss: 5.2515, Time/100 batches: 50.26 sec\nEpoch: 1, Step: 6150, Loss: 4.9067, Time/100 batches: 50.37 sec\nEpoch: 1, Step: 6300, Loss: 5.5038, Time/100 batches: 50.80 sec\nEpoch: 1, Step: 6450, Loss: 5.0181, Time/100 batches: 50.93 sec\nEpoch: 1, Step: 6600, Loss: 5.3450, Time/100 batches: 50.64 sec\nEpoch: 1, Step: 6750, Loss: 5.6908, Time/100 batches: 50.29 sec\nEpoch: 1, Step: 6900, Loss: 5.9580, Time/100 batches: 50.48 sec\nEpoch: 1, Step: 7050, Loss: 5.3892, Time/100 batches: 50.71 sec\nEpoch: 1, Step: 7200, Loss: 6.3516, Time/100 batches: 50.77 sec\nEpoch: 1, Step: 7350, Loss: 5.3684, Time/100 batches: 50.70 sec\nEpoch: 1, Step: 7500, Loss: 5.7232, Time/100 batches: 50.82 sec\nEpoch: 1, Step: 7650, Loss: 5.5719, Time/100 batches: 51.24 sec\nEpoch: 1, Step: 7800, Loss: 5.5200, Time/100 batches: 50.68 sec\nEpoch: 1, Step: 7950, Loss: 5.5118, Time/100 batches: 50.54 sec\nEpoch: 1, Step: 8100, Loss: 5.0799, Time/100 batches: 50.59 sec\nEpoch: 1, Step: 8250, Loss: 4.4050, Time/100 batches: 50.59 sec\nEpoch: 1, Step: 8400, Loss: 5.6323, Time/100 batches: 50.52 sec\nEpoch: 1, Step: 8550, Loss: 5.7393, Time/100 batches: 50.43 sec\nEpoch: 1, Step: 8700, Loss: 5.5419, Time/100 batches: 50.55 sec\nEpoch: 1, Step: 8850, Loss: 5.0488, Time/100 batches: 50.62 sec\nEpoch: 1, Step: 9000, Loss: 4.7830, Time/100 batches: 50.66 sec\nEpoch: 1, Step: 9150, Loss: 5.5642, Time/100 batches: 51.15 sec\nEpoch: 1, Step: 9300, Loss: 5.4660, Time/100 batches: 50.96 sec\nEpoch: 1, Step: 9450, Loss: 4.9351, Time/100 batches: 51.06 sec\nEpoch: 1, Step: 9600, Loss: 5.6997, Time/100 batches: 51.10 sec\nEpoch: 1, Step: 9750, Loss: 5.6175, Time/100 batches: 51.08 sec\nEpoch: 1, Step: 9900, Loss: 4.8346, Time/100 batches: 50.94 sec\nEpoch: 1, Step: 10050, Loss: 5.6609, Time/100 batches: 50.89 sec\nEpoch: 1, Step: 10200, Loss: 6.1848, Time/100 batches: 50.68 sec\nEpoch: 1, Step: 10350, Loss: 5.8136, Time/100 batches: 50.74 sec\nEpoch: 1, Step: 10500, Loss: 5.1019, Time/100 batches: 50.65 sec\nEpoch: 1, Step: 10650, Loss: 5.5192, Time/100 batches: 51.01 sec\nEpoch: 1, Step: 10800, Loss: 5.5026, Time/100 batches: 50.91 sec\nEpoch: 1, Step: 10950, Loss: 5.9348, Time/100 batches: 50.98 sec\nEpoch: 1, Step: 11100, Loss: 5.6738, Time/100 batches: 51.02 sec\nEpoch: 1, Step: 11250, Loss: 5.5039, Time/100 batches: 51.19 sec\nEpoch: 1, Step: 11400, Loss: 5.8034, Time/100 batches: 51.19 sec\nEpoch: 1, Step: 11550, Loss: 5.2979, Time/100 batches: 51.08 sec\nEpoch: 1, Step: 11700, Loss: 6.4328, Time/100 batches: 50.88 sec\nEpoch: 1, Step: 11850, Loss: 5.1144, Time/100 batches: 50.81 sec\nEpoch: 1, Step: 12000, Loss: 5.2534, Time/100 batches: 50.71 sec\nEpoch: 1, Step: 12150, Loss: 5.2057, Time/100 batches: 51.13 sec\nEpoch: 1, Step: 12300, Loss: 4.8912, Time/100 batches: 50.75 sec\nEpoch: 1, Step: 12450, Loss: 5.2084, Time/100 batches: 50.80 sec\nEpoch: 1, Step: 12600, Loss: 5.9092, Time/100 batches: 50.73 sec\nEpoch: 1, Step: 12750, Loss: 5.5088, Time/100 batches: 50.91 sec\nEpoch: 1, Step: 12900, Loss: 4.9219, Time/100 batches: 51.11 sec\nEpoch: 1, Step: 13050, Loss: 5.2714, Time/100 batches: 51.23 sec\nEpoch: 1, Step: 13200, Loss: 4.9158, Time/100 batches: 50.48 sec\nEpoch: 1, Step: 13350, Loss: 5.1901, Time/100 batches: 50.26 sec\nEpoch: 1, Step: 13500, Loss: 5.1564, Time/100 batches: 50.58 sec\nEpoch: 1, Step: 13650, Loss: 5.1758, Time/100 batches: 51.26 sec\nEpoch: 1, Step: 13800, Loss: 6.1975, Time/100 batches: 50.71 sec\nEpoch: 1, Step: 13950, Loss: 5.7894, Time/100 batches: 50.29 sec\nEpoch: 1, Step: 14100, Loss: 5.4962, Time/100 batches: 50.22 sec\nEpoch: 1, Step: 14250, Loss: 5.3870, Time/100 batches: 50.46 sec\nEpoch: 1, Step: 14400, Loss: 5.0685, Time/100 batches: 50.75 sec\nEpoch: 1, Step: 14550, Loss: 5.6178, Time/100 batches: 50.75 sec\nEpoch: 1, Step: 14700, Loss: 5.1408, Time/100 batches: 50.82 sec\nEpoch: 1, Step: 14850, Loss: 4.8267, Time/100 batches: 50.76 sec\nEpoch: 1, Step: 15000, Loss: 5.4432, Time/100 batches: 50.62 sec\nEpoch: 1, Step: 15150, Loss: 5.5430, Time/100 batches: 51.17 sec\nEpoch: 1, Step: 15300, Loss: 5.1619, Time/100 batches: 50.86 sec\nEpoch: 1, Step: 15450, Loss: 5.2744, Time/100 batches: 50.69 sec\nEpoch: 1, Step: 15600, Loss: 5.0647, Time/100 batches: 50.57 sec\nEpoch: 1, Step: 15750, Loss: 5.6325, Time/100 batches: 50.61 sec\nEpoch: 1, Step: 15900, Loss: 5.2477, Time/100 batches: 50.77 sec\nEpoch: 1, Step: 16050, Loss: 5.5871, Time/100 batches: 50.73 sec\nEpoch: 1, Step: 16200, Loss: 5.0728, Time/100 batches: 50.33 sec\nEpoch: 1, Step: 16350, Loss: 4.6417, Time/100 batches: 50.33 sec\nEpoch: 1, Step: 16500, Loss: 5.1773, Time/100 batches: 50.64 sec\nEpoch: 1, Step: 16650, Loss: 5.7129, Time/100 batches: 51.23 sec\nEpoch: 1, Step: 16800, Loss: 5.3582, Time/100 batches: 50.82 sec\nEpoch: 1, Step: 16950, Loss: 4.7831, Time/100 batches: 50.55 sec\nEpoch: 1, Step: 17100, Loss: 5.4076, Time/100 batches: 50.29 sec\nEpoch: 1, Step: 17250, Loss: 5.7794, Time/100 batches: 50.43 sec\nEpoch: 1, Step: 17400, Loss: 5.4667, Time/100 batches: 50.68 sec\nEpoch: 1, Step: 17550, Loss: 5.2872, Time/100 batches: 50.96 sec\nEpoch: 1, Step: 17700, Loss: 5.2040, Time/100 batches: 50.56 sec\nEpoch: 1, Step: 17850, Loss: 5.4344, Time/100 batches: 50.29 sec\nEpoch: 1, Step: 18000, Loss: 4.8655, Time/100 batches: 50.73 sec\nEpoch: 1, Step: 18150, Loss: 5.0520, Time/100 batches: 51.25 sec\nEpoch: 1, Step: 18300, Loss: 5.3318, Time/100 batches: 51.01 sec\nEpoch: 1, Step: 18450, Loss: 4.9230, Time/100 batches: 51.08 sec\nEpoch: 1, Step: 18600, Loss: 6.1424, Time/100 batches: 50.72 sec\nEpoch: 1, Step: 18750, Loss: 4.8764, Time/100 batches: 50.37 sec\nEpoch: 1, Step: 18900, Loss: 5.2907, Time/100 batches: 50.56 sec\nEpoch: 1, Step: 19050, Loss: 4.8198, Time/100 batches: 50.85 sec\nEpoch: 1, Step: 19200, Loss: 6.4729, Time/100 batches: 50.93 sec\nEpoch: 1, Step: 19350, Loss: 5.1053, Time/100 batches: 50.66 sec\nEpoch: 1, Step: 19500, Loss: 5.3323, Time/100 batches: 50.54 sec\nEpoch: 1, Step: 19650, Loss: 6.2004, Time/100 batches: 50.87 sec\nEpoch: 1, Step: 19800, Loss: 5.0365, Time/100 batches: 50.39 sec\nEpoch: 1, Step: 19950, Loss: 5.1289, Time/100 batches: 50.50 sec\nEpoch: 1, Step: 20100, Loss: 5.9387, Time/100 batches: 50.40 sec\nEpoch: 1, Step: 20250, Loss: 5.7896, Time/100 batches: 50.36 sec\nEpoch: 1, Step: 20400, Loss: 5.1921, Time/100 batches: 50.34 sec\nEpoch: 1, Step: 20550, Loss: 5.6896, Time/100 batches: 50.44 sec\nEpoch: 1, Step: 20700, Loss: 5.6446, Time/100 batches: 50.42 sec\nEpoch: 1, Step: 20850, Loss: 4.8433, Time/100 batches: 50.37 sec\nEpoch: 1, Step: 21000, Loss: 4.5929, Time/100 batches: 50.44 sec\nEpoch: 1, Step: 21150, Loss: 5.2710, Time/100 batches: 50.74 sec\nEpoch 1 total time: 7182.74 sec\nModel_0 Val Loss:5.205616313141662\nEpoch: 2, Step: 150, Loss: 4.1514, Time/100 batches: 50.41 sec\nEpoch: 2, Step: 300, Loss: 4.7919, Time/100 batches: 50.19 sec\nEpoch: 2, Step: 450, Loss: 5.0376, Time/100 batches: 50.48 sec\nEpoch: 2, Step: 600, Loss: 4.6454, Time/100 batches: 50.65 sec\nEpoch: 2, Step: 750, Loss: 5.6049, Time/100 batches: 50.88 sec\nEpoch: 2, Step: 900, Loss: 5.1804, Time/100 batches: 50.87 sec\nEpoch: 2, Step: 1050, Loss: 4.8553, Time/100 batches: 50.21 sec\nEpoch: 2, Step: 1200, Loss: 4.8856, Time/100 batches: 50.34 sec\nEpoch: 2, Step: 1350, Loss: 4.7571, Time/100 batches: 50.71 sec\nEpoch: 2, Step: 1500, Loss: 5.3759, Time/100 batches: 50.86 sec\nEpoch: 2, Step: 1650, Loss: 4.5773, Time/100 batches: 50.65 sec\nEpoch: 2, Step: 1800, Loss: 5.0641, Time/100 batches: 50.22 sec\nEpoch: 2, Step: 1950, Loss: 5.1029, Time/100 batches: 50.33 sec\nEpoch: 2, Step: 2100, Loss: 5.4299, Time/100 batches: 51.05 sec\nEpoch: 2, Step: 2250, Loss: 5.2227, Time/100 batches: 51.01 sec\nEpoch: 2, Step: 2400, Loss: 5.7665, Time/100 batches: 51.18 sec\nEpoch: 2, Step: 2550, Loss: 5.3986, Time/100 batches: 51.09 sec\nEpoch: 2, Step: 2700, Loss: 4.8345, Time/100 batches: 50.30 sec\nEpoch: 2, Step: 2850, Loss: 5.6277, Time/100 batches: 50.31 sec\nEpoch: 2, Step: 3000, Loss: 5.4079, Time/100 batches: 50.56 sec\nEpoch: 2, Step: 3150, Loss: 5.4756, Time/100 batches: 50.59 sec\nEpoch: 2, Step: 3300, Loss: 5.1630, Time/100 batches: 50.27 sec\nEpoch: 2, Step: 3450, Loss: 5.0085, Time/100 batches: 50.56 sec\nEpoch: 2, Step: 3600, Loss: 4.7205, Time/100 batches: 50.48 sec\nEpoch: 2, Step: 3750, Loss: 5.5042, Time/100 batches: 50.63 sec\nEpoch: 2, Step: 3900, Loss: 5.1497, Time/100 batches: 50.35 sec\nEpoch: 2, Step: 4050, Loss: 5.3978, Time/100 batches: 50.23 sec\nEpoch: 2, Step: 4200, Loss: 5.2802, Time/100 batches: 50.36 sec\nEpoch: 2, Step: 4350, Loss: 5.1611, Time/100 batches: 50.66 sec\nEpoch: 2, Step: 4500, Loss: 5.6714, Time/100 batches: 50.61 sec\nEpoch: 2, Step: 4650, Loss: 5.5826, Time/100 batches: 50.70 sec\nEpoch: 2, Step: 4800, Loss: 5.0865, Time/100 batches: 51.48 sec\nEpoch: 2, Step: 4950, Loss: 5.2428, Time/100 batches: 51.22 sec\nEpoch: 2, Step: 5100, Loss: 5.0507, Time/100 batches: 51.08 sec\nEpoch: 2, Step: 5250, Loss: 5.3185, Time/100 batches: 50.98 sec\nEpoch: 2, Step: 5400, Loss: 4.9796, Time/100 batches: 50.77 sec\nEpoch: 2, Step: 5550, Loss: 4.8716, Time/100 batches: 50.65 sec\nEpoch: 2, Step: 5700, Loss: 4.8951, Time/100 batches: 50.60 sec\nEpoch: 2, Step: 5850, Loss: 4.5920, Time/100 batches: 50.63 sec\nEpoch: 2, Step: 6000, Loss: 4.9538, Time/100 batches: 50.48 sec\nEpoch: 2, Step: 6150, Loss: 4.6730, Time/100 batches: 50.59 sec\nEpoch: 2, Step: 6300, Loss: 5.2715, Time/100 batches: 50.97 sec\nEpoch: 2, Step: 6450, Loss: 4.7595, Time/100 batches: 50.88 sec\nEpoch: 2, Step: 6600, Loss: 5.1259, Time/100 batches: 51.27 sec\nEpoch: 2, Step: 6750, Loss: 5.4169, Time/100 batches: 50.83 sec\nEpoch: 2, Step: 6900, Loss: 5.6839, Time/100 batches: 50.19 sec\nEpoch: 2, Step: 7050, Loss: 5.2669, Time/100 batches: 50.24 sec\nEpoch: 2, Step: 7200, Loss: 6.1072, Time/100 batches: 50.39 sec\nEpoch: 2, Step: 7350, Loss: 5.1692, Time/100 batches: 50.47 sec\nEpoch: 2, Step: 7500, Loss: 5.3630, Time/100 batches: 50.59 sec\nEpoch: 2, Step: 7650, Loss: 5.2849, Time/100 batches: 50.95 sec\nEpoch: 2, Step: 7800, Loss: 5.3134, Time/100 batches: 50.49 sec\nEpoch: 2, Step: 7950, Loss: 5.1974, Time/100 batches: 50.53 sec\nEpoch: 2, Step: 8100, Loss: 4.8350, Time/100 batches: 50.53 sec\nEpoch: 2, Step: 8250, Loss: 4.2133, Time/100 batches: 50.54 sec\nEpoch: 2, Step: 8400, Loss: 5.3793, Time/100 batches: 50.69 sec\nEpoch: 2, Step: 8550, Loss: 5.4796, Time/100 batches: 50.75 sec\nEpoch: 2, Step: 8700, Loss: 5.2337, Time/100 batches: 50.77 sec\nEpoch: 2, Step: 8850, Loss: 4.7911, Time/100 batches: 50.92 sec\nEpoch: 2, Step: 9000, Loss: 4.5626, Time/100 batches: 50.91 sec\nEpoch: 2, Step: 9150, Loss: 5.3753, Time/100 batches: 51.52 sec\nEpoch: 2, Step: 9300, Loss: 5.1898, Time/100 batches: 50.87 sec\nEpoch: 2, Step: 9450, Loss: 4.7331, Time/100 batches: 50.45 sec\nEpoch: 2, Step: 9600, Loss: 5.4591, Time/100 batches: 50.39 sec\nEpoch: 2, Step: 9750, Loss: 5.2943, Time/100 batches: 50.67 sec\nEpoch: 2, Step: 9900, Loss: 4.6512, Time/100 batches: 50.61 sec\nEpoch: 2, Step: 10050, Loss: 5.4512, Time/100 batches: 50.40 sec\nEpoch: 2, Step: 10200, Loss: 5.9294, Time/100 batches: 50.49 sec\nEpoch: 2, Step: 10350, Loss: 5.6095, Time/100 batches: 50.49 sec\nEpoch: 2, Step: 10500, Loss: 4.9259, Time/100 batches: 50.63 sec\nEpoch: 2, Step: 10650, Loss: 5.2427, Time/100 batches: 51.03 sec\nEpoch: 2, Step: 10800, Loss: 5.2665, Time/100 batches: 50.91 sec\nEpoch: 2, Step: 10950, Loss: 5.6495, Time/100 batches: 50.62 sec\nEpoch: 2, Step: 11100, Loss: 5.4787, Time/100 batches: 50.65 sec\nEpoch: 2, Step: 11250, Loss: 5.2958, Time/100 batches: 50.78 sec\nEpoch: 2, Step: 11400, Loss: 5.5871, Time/100 batches: 50.97 sec\nEpoch: 2, Step: 11550, Loss: 5.0913, Time/100 batches: 50.60 sec\nEpoch: 2, Step: 11700, Loss: 6.1108, Time/100 batches: 50.34 sec\nEpoch: 2, Step: 11850, Loss: 4.8901, Time/100 batches: 50.62 sec\nEpoch: 2, Step: 12000, Loss: 5.0593, Time/100 batches: 50.74 sec\nEpoch: 2, Step: 12150, Loss: 5.0023, Time/100 batches: 51.15 sec\nEpoch: 2, Step: 12300, Loss: 4.6333, Time/100 batches: 50.57 sec\nEpoch: 2, Step: 12450, Loss: 5.0024, Time/100 batches: 50.52 sec\nEpoch: 2, Step: 12600, Loss: 5.6697, Time/100 batches: 50.58 sec\nEpoch: 2, Step: 12750, Loss: 5.2982, Time/100 batches: 50.55 sec\nEpoch: 2, Step: 12900, Loss: 4.6963, Time/100 batches: 50.86 sec\nEpoch: 2, Step: 13050, Loss: 5.0758, Time/100 batches: 50.61 sec\nEpoch: 2, Step: 13200, Loss: 4.6728, Time/100 batches: 50.61 sec\nEpoch: 2, Step: 13350, Loss: 4.9448, Time/100 batches: 50.50 sec\nEpoch: 2, Step: 13500, Loss: 4.9481, Time/100 batches: 50.54 sec\nEpoch: 2, Step: 13650, Loss: 4.9943, Time/100 batches: 50.95 sec\nEpoch: 2, Step: 13800, Loss: 5.8072, Time/100 batches: 50.62 sec\nEpoch: 2, Step: 13950, Loss: 5.5810, Time/100 batches: 50.74 sec\nEpoch: 2, Step: 14100, Loss: 5.3097, Time/100 batches: 50.99 sec\nEpoch: 2, Step: 14250, Loss: 5.1653, Time/100 batches: 50.66 sec\nEpoch: 2, Step: 14400, Loss: 4.9248, Time/100 batches: 50.24 sec\nEpoch: 2, Step: 14550, Loss: 5.4020, Time/100 batches: 50.10 sec\nEpoch: 2, Step: 14700, Loss: 4.9213, Time/100 batches: 50.07 sec\nEpoch: 2, Step: 14850, Loss: 4.6439, Time/100 batches: 50.07 sec\nEpoch: 2, Step: 15000, Loss: 5.2613, Time/100 batches: 50.02 sec\nEpoch: 2, Step: 15150, Loss: 5.3484, Time/100 batches: 50.82 sec\nEpoch: 2, Step: 15300, Loss: 5.0025, Time/100 batches: 50.31 sec\nEpoch: 2, Step: 15450, Loss: 5.0943, Time/100 batches: 50.23 sec\nEpoch: 2, Step: 15600, Loss: 4.8750, Time/100 batches: 50.16 sec\nEpoch: 2, Step: 15750, Loss: 5.4564, Time/100 batches: 50.19 sec\nEpoch: 2, Step: 15900, Loss: 5.0627, Time/100 batches: 50.17 sec\nEpoch: 2, Step: 16050, Loss: 5.4102, Time/100 batches: 50.16 sec\nEpoch: 2, Step: 16200, Loss: 4.8765, Time/100 batches: 50.19 sec\nEpoch: 2, Step: 16350, Loss: 4.4513, Time/100 batches: 50.12 sec\nEpoch: 2, Step: 16500, Loss: 4.9880, Time/100 batches: 50.11 sec\nEpoch: 2, Step: 16650, Loss: 5.4004, Time/100 batches: 50.51 sec\nEpoch: 2, Step: 16800, Loss: 5.1782, Time/100 batches: 50.37 sec\nEpoch: 2, Step: 16950, Loss: 4.6088, Time/100 batches: 50.43 sec\nEpoch: 2, Step: 17100, Loss: 5.2635, Time/100 batches: 50.23 sec\nEpoch: 2, Step: 17250, Loss: 5.5756, Time/100 batches: 50.14 sec\nEpoch: 2, Step: 17400, Loss: 5.3345, Time/100 batches: 50.08 sec\nEpoch: 2, Step: 17550, Loss: 5.0262, Time/100 batches: 50.10 sec\nEpoch: 2, Step: 17700, Loss: 5.0420, Time/100 batches: 50.17 sec\nEpoch: 2, Step: 17850, Loss: 5.2521, Time/100 batches: 50.12 sec\nEpoch: 2, Step: 18000, Loss: 4.6873, Time/100 batches: 50.12 sec\nEpoch: 2, Step: 18150, Loss: 4.8993, Time/100 batches: 50.76 sec\nEpoch: 2, Step: 18300, Loss: 5.1806, Time/100 batches: 50.83 sec\nEpoch: 2, Step: 18450, Loss: 4.7639, Time/100 batches: 50.32 sec\nEpoch: 2, Step: 18600, Loss: 5.9160, Time/100 batches: 50.11 sec\nEpoch: 2, Step: 18750, Loss: 4.7017, Time/100 batches: 50.09 sec\nEpoch: 2, Step: 18900, Loss: 5.1269, Time/100 batches: 50.28 sec\nEpoch: 2, Step: 19050, Loss: 4.6484, Time/100 batches: 50.54 sec\nEpoch: 2, Step: 19200, Loss: 6.3054, Time/100 batches: 50.35 sec\nEpoch: 2, Step: 19350, Loss: 4.8876, Time/100 batches: 50.03 sec\nEpoch: 2, Step: 19500, Loss: 5.1545, Time/100 batches: 49.91 sec\nEpoch: 2, Step: 19650, Loss: 6.0201, Time/100 batches: 50.38 sec\nEpoch: 2, Step: 19800, Loss: 4.9011, Time/100 batches: 50.13 sec\nEpoch: 2, Step: 19950, Loss: 4.9983, Time/100 batches: 50.26 sec\nEpoch: 2, Step: 20100, Loss: 5.5534, Time/100 batches: 50.32 sec\nEpoch: 2, Step: 20250, Loss: 5.5885, Time/100 batches: 50.25 sec\nEpoch: 2, Step: 20400, Loss: 5.0573, Time/100 batches: 50.09 sec\nEpoch: 2, Step: 20550, Loss: 5.4664, Time/100 batches: 50.01 sec\nEpoch: 2, Step: 20700, Loss: 5.4876, Time/100 batches: 49.91 sec\nEpoch: 2, Step: 20850, Loss: 4.7202, Time/100 batches: 49.94 sec\nEpoch: 2, Step: 21000, Loss: 4.4747, Time/100 batches: 49.96 sec\nEpoch: 2, Step: 21150, Loss: 5.1334, Time/100 batches: 50.33 sec\nEpoch 2 total time: 7159.42 sec\nModel_1 Val Loss:5.06194425558115\nEpoch: 3, Step: 150, Loss: 4.0398, Time/100 batches: 50.16 sec\nEpoch: 3, Step: 300, Loss: 4.7112, Time/100 batches: 50.36 sec\nEpoch: 3, Step: 450, Loss: 4.9425, Time/100 batches: 50.22 sec\nEpoch: 3, Step: 600, Loss: 4.5719, Time/100 batches: 50.24 sec\nEpoch: 3, Step: 750, Loss: 5.5126, Time/100 batches: 50.21 sec\nEpoch: 3, Step: 900, Loss: 5.0760, Time/100 batches: 50.53 sec\nEpoch: 3, Step: 1050, Loss: 4.7682, Time/100 batches: 50.16 sec\nEpoch: 3, Step: 1200, Loss: 4.7857, Time/100 batches: 50.20 sec\nEpoch: 3, Step: 1350, Loss: 4.6735, Time/100 batches: 50.22 sec\nEpoch: 3, Step: 1500, Loss: 5.2693, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 1650, Loss: 4.4448, Time/100 batches: 50.45 sec\nEpoch: 3, Step: 1800, Loss: 4.9264, Time/100 batches: 50.41 sec\nEpoch: 3, Step: 1950, Loss: 4.8887, Time/100 batches: 50.20 sec\nEpoch: 3, Step: 2100, Loss: 5.2761, Time/100 batches: 50.59 sec\nEpoch: 3, Step: 2250, Loss: 5.0559, Time/100 batches: 50.18 sec\nEpoch: 3, Step: 2400, Loss: 5.6069, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 2550, Loss: 5.2519, Time/100 batches: 50.12 sec\nEpoch: 3, Step: 2700, Loss: 4.7023, Time/100 batches: 50.01 sec\nEpoch: 3, Step: 2850, Loss: 5.4752, Time/100 batches: 50.05 sec\nEpoch: 3, Step: 3000, Loss: 5.2546, Time/100 batches: 50.13 sec\nEpoch: 3, Step: 3150, Loss: 5.3318, Time/100 batches: 50.32 sec\nEpoch: 3, Step: 3300, Loss: 5.0204, Time/100 batches: 50.25 sec\nEpoch: 3, Step: 3450, Loss: 4.8729, Time/100 batches: 50.55 sec\nEpoch: 3, Step: 3600, Loss: 4.6176, Time/100 batches: 50.09 sec\nEpoch: 3, Step: 3750, Loss: 5.3569, Time/100 batches: 49.97 sec\nEpoch: 3, Step: 3900, Loss: 5.0070, Time/100 batches: 50.10 sec\nEpoch: 3, Step: 4050, Loss: 5.2352, Time/100 batches: 50.05 sec\nEpoch: 3, Step: 4200, Loss: 5.1136, Time/100 batches: 50.04 sec\nEpoch: 3, Step: 4350, Loss: 5.0282, Time/100 batches: 50.11 sec\nEpoch: 3, Step: 4500, Loss: 5.5306, Time/100 batches: 50.10 sec\nEpoch: 3, Step: 4650, Loss: 5.4509, Time/100 batches: 50.27 sec\nEpoch: 3, Step: 4800, Loss: 4.9631, Time/100 batches: 50.64 sec\nEpoch: 3, Step: 4950, Loss: 5.0829, Time/100 batches: 50.35 sec\nEpoch: 3, Step: 5100, Loss: 4.8783, Time/100 batches: 50.51 sec\nEpoch: 3, Step: 5250, Loss: 5.1761, Time/100 batches: 50.14 sec\nEpoch: 3, Step: 5400, Loss: 4.8444, Time/100 batches: 50.07 sec\nEpoch: 3, Step: 5550, Loss: 4.7295, Time/100 batches: 50.02 sec\nEpoch: 3, Step: 5700, Loss: 4.7523, Time/100 batches: 50.06 sec\nEpoch: 3, Step: 5850, Loss: 4.4823, Time/100 batches: 50.07 sec\nEpoch: 3, Step: 6000, Loss: 4.8360, Time/100 batches: 50.12 sec\nEpoch: 3, Step: 6150, Loss: 4.5382, Time/100 batches: 50.25 sec\nEpoch: 3, Step: 6300, Loss: 5.1652, Time/100 batches: 50.60 sec\nEpoch: 3, Step: 6450, Loss: 4.6363, Time/100 batches: 50.39 sec\nEpoch: 3, Step: 6600, Loss: 5.0061, Time/100 batches: 50.66 sec\nEpoch: 3, Step: 6750, Loss: 5.2915, Time/100 batches: 50.58 sec\nEpoch: 3, Step: 6900, Loss: 5.4984, Time/100 batches: 50.66 sec\nEpoch: 3, Step: 7050, Loss: 5.2072, Time/100 batches: 50.31 sec\nEpoch: 3, Step: 7200, Loss: 5.9745, Time/100 batches: 50.12 sec\nEpoch: 3, Step: 7350, Loss: 5.0738, Time/100 batches: 50.07 sec\nEpoch: 3, Step: 7500, Loss: 5.1872, Time/100 batches: 50.05 sec\nEpoch: 3, Step: 7650, Loss: 5.1243, Time/100 batches: 50.47 sec\nEpoch: 3, Step: 7800, Loss: 5.1855, Time/100 batches: 50.13 sec\nEpoch: 3, Step: 7950, Loss: 5.0384, Time/100 batches: 50.19 sec\nEpoch: 3, Step: 8100, Loss: 4.7123, Time/100 batches: 50.28 sec\nEpoch: 3, Step: 8250, Loss: 4.1072, Time/100 batches: 50.23 sec\nEpoch: 3, Step: 8400, Loss: 5.2310, Time/100 batches: 50.18 sec\nEpoch: 3, Step: 8550, Loss: 5.3577, Time/100 batches: 50.13 sec\nEpoch: 3, Step: 8700, Loss: 5.0682, Time/100 batches: 50.13 sec\nEpoch: 3, Step: 8850, Loss: 4.6506, Time/100 batches: 50.10 sec\nEpoch: 3, Step: 9000, Loss: 4.4719, Time/100 batches: 50.09 sec\nEpoch: 3, Step: 9150, Loss: 5.2776, Time/100 batches: 50.46 sec\nEpoch: 3, Step: 9300, Loss: 5.0631, Time/100 batches: 50.09 sec\nEpoch: 3, Step: 9450, Loss: 4.6223, Time/100 batches: 50.10 sec\nEpoch: 3, Step: 9600, Loss: 5.3456, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 9750, Loss: 5.1318, Time/100 batches: 50.20 sec\nEpoch: 3, Step: 9900, Loss: 4.5685, Time/100 batches: 50.26 sec\nEpoch: 3, Step: 10050, Loss: 5.3584, Time/100 batches: 50.41 sec\nEpoch: 3, Step: 10200, Loss: 5.7888, Time/100 batches: 50.46 sec\nEpoch: 3, Step: 10350, Loss: 5.5053, Time/100 batches: 50.58 sec\nEpoch: 3, Step: 10500, Loss: 4.8244, Time/100 batches: 50.61 sec\nEpoch: 3, Step: 10650, Loss: 5.0676, Time/100 batches: 50.99 sec\nEpoch: 3, Step: 10800, Loss: 5.1392, Time/100 batches: 50.61 sec\nEpoch: 3, Step: 10950, Loss: 5.5006, Time/100 batches: 50.54 sec\nEpoch: 3, Step: 11100, Loss: 5.3623, Time/100 batches: 50.53 sec\nEpoch: 3, Step: 11250, Loss: 5.1579, Time/100 batches: 50.53 sec\nEpoch: 3, Step: 11400, Loss: 5.4642, Time/100 batches: 50.48 sec\nEpoch: 3, Step: 11550, Loss: 4.9781, Time/100 batches: 50.59 sec\nEpoch: 3, Step: 11700, Loss: 5.9259, Time/100 batches: 50.71 sec\nEpoch: 3, Step: 11850, Loss: 4.7644, Time/100 batches: 50.64 sec\nEpoch: 3, Step: 12000, Loss: 4.9584, Time/100 batches: 50.43 sec\nEpoch: 3, Step: 12150, Loss: 4.8931, Time/100 batches: 50.80 sec\nEpoch: 3, Step: 12300, Loss: 4.5137, Time/100 batches: 50.42 sec\nEpoch: 3, Step: 12450, Loss: 4.8776, Time/100 batches: 50.40 sec\nEpoch: 3, Step: 12600, Loss: 5.5388, Time/100 batches: 50.50 sec\nEpoch: 3, Step: 12750, Loss: 5.1762, Time/100 batches: 50.60 sec\nEpoch: 3, Step: 12900, Loss: 4.5779, Time/100 batches: 50.59 sec\nEpoch: 3, Step: 13050, Loss: 4.9639, Time/100 batches: 50.58 sec\nEpoch: 3, Step: 13200, Loss: 4.5511, Time/100 batches: 50.48 sec\nEpoch: 3, Step: 13350, Loss: 4.8171, Time/100 batches: 50.32 sec\nEpoch: 3, Step: 13500, Loss: 4.8300, Time/100 batches: 50.44 sec\nEpoch: 3, Step: 13650, Loss: 4.8843, Time/100 batches: 50.89 sec\nEpoch: 3, Step: 13800, Loss: 5.5635, Time/100 batches: 50.50 sec\nEpoch: 3, Step: 13950, Loss: 5.4783, Time/100 batches: 50.56 sec\nEpoch: 3, Step: 14100, Loss: 5.2038, Time/100 batches: 50.36 sec\nEpoch: 3, Step: 14250, Loss: 5.0533, Time/100 batches: 50.16 sec\nEpoch: 3, Step: 14400, Loss: 4.8400, Time/100 batches: 50.05 sec\nEpoch: 3, Step: 14550, Loss: 5.2855, Time/100 batches: 50.06 sec\nEpoch: 3, Step: 14700, Loss: 4.7984, Time/100 batches: 50.08 sec\nEpoch: 3, Step: 14850, Loss: 4.5315, Time/100 batches: 50.00 sec\nEpoch: 3, Step: 15000, Loss: 5.1651, Time/100 batches: 50.09 sec\nEpoch: 3, Step: 15150, Loss: 5.2091, Time/100 batches: 50.63 sec\nEpoch: 3, Step: 15300, Loss: 4.8988, Time/100 batches: 50.48 sec\nEpoch: 3, Step: 15450, Loss: 4.9823, Time/100 batches: 50.53 sec\nEpoch: 3, Step: 15600, Loss: 4.7609, Time/100 batches: 50.66 sec\nEpoch: 3, Step: 15750, Loss: 5.3557, Time/100 batches: 50.29 sec\nEpoch: 3, Step: 15900, Loss: 4.9465, Time/100 batches: 50.16 sec\nEpoch: 3, Step: 16050, Loss: 5.3168, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 16200, Loss: 4.7527, Time/100 batches: 50.10 sec\nEpoch: 3, Step: 16350, Loss: 4.3310, Time/100 batches: 50.25 sec\nEpoch: 3, Step: 16500, Loss: 4.8812, Time/100 batches: 50.38 sec\nEpoch: 3, Step: 16650, Loss: 5.2150, Time/100 batches: 50.88 sec\nEpoch: 3, Step: 16800, Loss: 5.0770, Time/100 batches: 50.53 sec\nEpoch: 3, Step: 16950, Loss: 4.5171, Time/100 batches: 50.57 sec\nEpoch: 3, Step: 17100, Loss: 5.1683, Time/100 batches: 50.36 sec\nEpoch: 3, Step: 17250, Loss: 5.4630, Time/100 batches: 50.12 sec\nEpoch: 3, Step: 17400, Loss: 5.2315, Time/100 batches: 50.14 sec\nEpoch: 3, Step: 17550, Loss: 4.8850, Time/100 batches: 50.26 sec\nEpoch: 3, Step: 17700, Loss: 4.9474, Time/100 batches: 50.42 sec\nEpoch: 3, Step: 17850, Loss: 5.1542, Time/100 batches: 50.60 sec\nEpoch: 3, Step: 18000, Loss: 4.5925, Time/100 batches: 50.76 sec\nEpoch: 3, Step: 18150, Loss: 4.7969, Time/100 batches: 51.11 sec\nEpoch: 3, Step: 18300, Loss: 5.0766, Time/100 batches: 50.69 sec\nEpoch: 3, Step: 18450, Loss: 4.6546, Time/100 batches: 50.66 sec\nEpoch: 3, Step: 18600, Loss: 5.7824, Time/100 batches: 50.74 sec\nEpoch: 3, Step: 18750, Loss: 4.6059, Time/100 batches: 50.79 sec\nEpoch: 3, Step: 18900, Loss: 5.0270, Time/100 batches: 50.47 sec\nEpoch: 3, Step: 19050, Loss: 4.5554, Time/100 batches: 50.33 sec\nEpoch: 3, Step: 19200, Loss: 6.1824, Time/100 batches: 50.40 sec\nEpoch: 3, Step: 19350, Loss: 4.7542, Time/100 batches: 50.23 sec\nEpoch: 3, Step: 19500, Loss: 5.0417, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 19650, Loss: 5.9282, Time/100 batches: 50.55 sec\nEpoch: 3, Step: 19800, Loss: 4.8056, Time/100 batches: 50.31 sec\nEpoch: 3, Step: 19950, Loss: 4.9116, Time/100 batches: 50.25 sec\nEpoch: 3, Step: 20100, Loss: 5.3734, Time/100 batches: 50.17 sec\nEpoch: 3, Step: 20250, Loss: 5.4500, Time/100 batches: 50.08 sec\nEpoch: 3, Step: 20400, Loss: 4.9788, Time/100 batches: 50.02 sec\nEpoch: 3, Step: 20550, Loss: 5.3325, Time/100 batches: 49.91 sec\nEpoch: 3, Step: 20700, Loss: 5.3819, Time/100 batches: 49.88 sec\nEpoch: 3, Step: 20850, Loss: 4.6507, Time/100 batches: 50.02 sec\nEpoch: 3, Step: 21000, Loss: 4.4104, Time/100 batches: 50.15 sec\nEpoch: 3, Step: 21150, Loss: 5.0461, Time/100 batches: 50.61 sec\nEpoch 3 total time: 7133.29 sec\nModel_2 Val Loss:4.981067570773038\nEpoch: 4, Step: 150, Loss: 3.9615, Time/100 batches: 50.01 sec\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3561993389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":44},{"cell_type":"code","source":"# Now to finetune the model to proper english grammer while having a similar vocabular, ill be fine-tuning it on \n# the poem's summary, jus 10% of the total summary, jus to tweak/guide the model in the direction not completely\n# change the generation\ndf_ft=pd.read_csv(\"/kaggle/working/poemDatasetWithSummary.csv\")\nx=df_ft[\"jist\"].tolist()\nids_ft=[]\nfor i in x:\n    ids_ft.extend(tokenizer.encode(i).ids)\nids_ft=ids_ft[:30000]\nids_ft=torch.tensor(ids_ft,dtype=torch.long).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:05:52.649656Z","iopub.execute_input":"2026-01-31T13:05:52.650216Z","iopub.status.idle":"2026-01-31T13:05:53.333262Z","shell.execute_reply.started":"2026-01-31T13:05:52.650187Z","shell.execute_reply":"2026-01-31T13:05:53.332677Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"len(ids_ft)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:05:55.155865Z","iopub.execute_input":"2026-01-31T13:05:55.156166Z","iopub.status.idle":"2026-01-31T13:05:55.161306Z","shell.execute_reply.started":"2026-01-31T13:05:55.156140Z","shell.execute_reply":"2026-01-31T13:05:55.160618Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"tokenizer=Tokenizer.from_file(\"Tokenizor.json\")\nsd=torch.load(\"Mmodel_epoch__3.pt\",map_location=device)\nsd={k.replace(\"_orig_mod.\",\"\"):v for k,v in sd.items()}\nmodel_ft=GPT()\nmodel_ft.load_state_dict(sd)\nmodel_ft.to(device)\nmodel_ft.eval()\nprint(\"Model Loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:06:06.731869Z","iopub.execute_input":"2026-01-31T13:06:06.732174Z","iopub.status.idle":"2026-01-31T13:06:06.990730Z","shell.execute_reply.started":"2026-01-31T13:06:06.732148Z","shell.execute_reply":"2026-01-31T13:06:06.989886Z"}},"outputs":[{"name":"stdout","text":"Model Loaded\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"optimizer_ft=torch.optim.AdamW(model_ft.parameters(),lr=lr_ft)\ncriterion=nn.CrossEntropyLoss()\nepochs=2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:07:44.105484Z","iopub.execute_input":"2026-01-31T13:07:44.106070Z","iopub.status.idle":"2026-01-31T13:07:44.111102Z","shell.execute_reply.started":"2026-01-31T13:07:44.106040Z","shell.execute_reply":"2026-01-31T13:07:44.110513Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"for i in range(epochs):\n    step=0\n    curr=time.time()\n    final=curr\n    for x,y in generator(ids_ft,batch_size,context_window_length):\n        optimizer.zero_grad(set_to_none=True)\n        logits=model_ft(x)\n        logits=logits.view(-1,logits.shape[-1])\n        y=y.view(-1)\n\n        loss=criterion(logits,y)\n        loss.backward()\n        optimizer.step()\n\n        step+=1\n\n        if step%10==0:\n            final=time.time()\n            print(\"Loss:\",loss.item(),\"Time:\",final-curr)\n            curr=final\n\nprint(\"Finetuning Done\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:08:54.006885Z","iopub.execute_input":"2026-01-31T13:08:54.007763Z","iopub.status.idle":"2026-01-31T13:13:04.818541Z","shell.execute_reply.started":"2026-01-31T13:08:54.007733Z","shell.execute_reply":"2026-01-31T13:13:04.817850Z"}},"outputs":[{"name":"stdout","text":"Loss: 5.310453414916992 Time: 10.943147659301758\nLoss: 5.0175065994262695 Time: 11.836565017700195\nLoss: 5.494462966918945 Time: 11.309890508651733\nLoss: 4.942111015319824 Time: 10.757989168167114\nLoss: 5.405850410461426 Time: 10.474914073944092\nLoss: 5.486783027648926 Time: 10.373248815536499\nLoss: 4.994131565093994 Time: 10.458373308181763\nLoss: 5.574784755706787 Time: 10.645584344863892\nLoss: 5.264217376708984 Time: 10.85484004020691\nLoss: 5.462823867797852 Time: 10.938098192214966\nLoss: 5.168017387390137 Time: 10.919804573059082\nLoss: 5.310453414916992 Time: 10.7510244846344\nLoss: 5.0175065994262695 Time: 10.662105083465576\nLoss: 5.494462966918945 Time: 10.652871131896973\nLoss: 4.942111015319824 Time: 10.712511539459229\nLoss: 5.405850410461426 Time: 10.746970176696777\nLoss: 5.486783027648926 Time: 10.758150100708008\nLoss: 4.994131565093994 Time: 10.791898250579834\nLoss: 5.574784755706787 Time: 10.804127216339111\nLoss: 5.264217376708984 Time: 10.820141315460205\nLoss: 5.462823867797852 Time: 10.82758355140686\nLoss: 5.168017387390137 Time: 10.791552305221558\nFinetuning Done\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"torch.save(model_ft.state_dict(),\"Model_FineTuned.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T13:13:12.711339Z","iopub.execute_input":"2026-01-31T13:13:12.712105Z","iopub.status.idle":"2026-01-31T13:13:12.796488Z","shell.execute_reply.started":"2026-01-31T13:13:12.712073Z","shell.execute_reply":"2026-01-31T13:13:12.795905Z"}},"outputs":[],"execution_count":79}]}