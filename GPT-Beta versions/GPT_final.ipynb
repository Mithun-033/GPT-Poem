{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2tjYJUz7MFA",
        "outputId": "47a1cf8f-d548-4902-ecda-e8d3cdca882f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfodthzdt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcNemdfctU_w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yhSau1s8ZfNA",
        "outputId": "ee84ad79-005d-45bf-95e9-d93558628e37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiHWwhB0togE",
        "outputId": "e3435dd8-4aca-4f05-ca45-775d7f2db152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 10.3M  100 10.3M    0     0  4382k      0  0:00:02  0:00:02 --:--:-- 7459k\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o poem-dataset.zip \\\n",
        "https://www.kaggle.com/api/v1/datasets/download/marufchowdhury/poem-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1boynZ04t9vB",
        "outputId": "87687e7d-5a61-469b-9593-956ecfef7dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  poem-dataset.zip\n",
            "replace Poems_Dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace poemDatasetWithSummary.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip poem-dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUbDM-EOuWcY"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Poems_Dataset.csv\")\n",
        "df=df[\"Poem Content\"]\n",
        "data=df.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRU-IZWrugfs"
      },
      "outputs": [],
      "source": [
        "#Hyper Parameters\n",
        "context_window_length=100\n",
        "batch_size=400\n",
        "n_embed=288\n",
        "n_head=9\n",
        "n_layers=8\n",
        "v_size=5000\n",
        "head_size=n_embed//n_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L-z8OqVmac5"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer=Whitespace()\n",
        "trainer=BpeTrainer(vocab_size=v_size)\n",
        "tokenizer.train_from_iterator(data,trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uilE4AdCoxN7",
        "outputId": "264f84cb-801f-4898-fe83-0da3ff6ece0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['hell', 'o', 'my', 'gu', 'y', 'how', 'are', 'you'],\n",
              " [3825, 78, 2059, 2571, 88, 2250, 2084, 2042])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out=tokenizer.encode(\"hello my guy how are you\")\n",
        "out.tokens,out.ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9gzukuMDzFG"
      },
      "outputs": [],
      "source": [
        "all_ids=[]\n",
        "\n",
        "for s in data:\n",
        "    all_ids.extend(tokenizer.encode(s).ids)\n",
        "\n",
        "ids=torch.tensor(all_ids,dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o27nzmcAco9N",
        "outputId": "7ac849e7-e5e2-4ecb-c7ea-fd093aea6d3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5344321"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCPKXJu9E7_4",
        "outputId": "96a7dfb2-35c1-4e36-beeb-5cd50cc4f6ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "ids=ids[:200000]\n",
        "len(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "907QdFNEuzYO"
      },
      "outputs": [],
      "source": [
        "def generator(ids,batch_size,cwl):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    count=0\n",
        "\n",
        "    for i in range(len(ids)-cwl):\n",
        "        X.append(ids[i:i+cwl])\n",
        "        Y.append(ids[i+1:i+cwl+1])\n",
        "        count+=1\n",
        "\n",
        "        if count==batch_size:\n",
        "            yield torch.stack(X).to(device),torch.stack(Y).to(device)\n",
        "            X=[]\n",
        "            Y=[]\n",
        "            count=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI8G4WR2rku9"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key=nn.Linear(n_embed,head_size) #(B,T,C)-->(B,T,H)\n",
        "        self.query=nn.Linear(n_embed,head_size) #(B,T,C)-->(B,T,H)\n",
        "        self.value=nn.Linear(n_embed,head_size)  #(B,T,C)-->(B,T,H)\n",
        "\n",
        "    def forward(self,x):\n",
        "        k=self.key(x)     #(B,T,H)\n",
        "        q=self.query(x)   #(B,T,H)\n",
        "        v=self.value(x)   #(B,T,H)\n",
        "\n",
        "        # Do Dot product of k and q\n",
        "\n",
        "        weights=k@q.transpose(-2,-1)*head_size**-0.5  # (B,T,H) x (B,H,T) --> (B,T,T)\n",
        "        T=x.size(1)\n",
        "        mask=torch.tril(torch.ones(T,T,device=x.device))\n",
        "        weights=weights.masked_fill(mask==0,float('-inf'))\n",
        "        weights=nn.functional.softmax(weights,dim=-1)\n",
        "        dropout = nn.Dropout(0.1)\n",
        "        weights = dropout(weights)\n",
        "\n",
        "        output=weights@v #(B,T,T) x (B,T,H) --> (B,T,H)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_ekLv4AxyUy"
      },
      "outputs": [],
      "source": [
        "class MultiHead(nn.Module):\n",
        "    def __init__(self,n_head,head_size):\n",
        "        super().__init__()\n",
        "        self.heads=nn.ModuleList([AttentionHead(head_size) for _ in range(n_head)])\n",
        "        self.project=nn.Linear(n_head*head_size,n_embed)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=torch.cat([h(x) for h in self.heads],dim=-1)  # (B,T,H*N)\n",
        "        #out=self.project(out)  # (B,T,H*N) --> (B,T,C)\n",
        "        dropout = nn.Dropout(0.1)\n",
        "        out = dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SkDUQSkzaDf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.FF=nn.Sequential(\n",
        "            nn.Linear(n_embed,3*n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(3*n_embed,n_embed),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.FF(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvYtXEmo05oI"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,n_embed,n_head):\n",
        "        super().__init__()\n",
        "        head_size=n_embed//n_head\n",
        "        self.SelfAtt = MultiHead(n_head, head_size)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + self.SelfAtt(self.ln1(x)) + self.ffwd(self.ln2(x))\n",
        "        return x  #(B,T,C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuWPQlPN21Pf"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed=nn.Embedding(v_size,n_embed)  # (B,T) --> (B,T,C)\n",
        "        self.pos_embed=nn.Embedding(context_window_length,n_embed) # (T) --> (T,C)\n",
        "\n",
        "        self.blocks=nn.Sequential(*[Block(n_embed,n_head) for _ in range(n_layers)])\n",
        "        self.final_layernorm = nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, v_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x ==> (B,T)\n",
        "\n",
        "        tok_embeds=self.embed(x) # (B,T,C)\n",
        "        pos_embeds=self.pos_embed(torch.arange(x.size(1),device=x.device)) #(T,C)\n",
        "        x=tok_embeds + pos_embeds # pos_embed r broadcasted and added to every batch element\n",
        "\n",
        "        x=self.blocks(x)\n",
        "        x=self.final_layernorm(x)\n",
        "        logits=self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(model,idx,max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            if idx.size(1)>context_window_length:\n",
        "                idx_cond=idx[:,-context_window_length:]\n",
        "            else:\n",
        "                idx_cond=idx\n",
        "\n",
        "            logits=model(idx_cond)\n",
        "            probs=torch.softmax(logits[:,-1,:],dim=-1)\n",
        "            next_token=torch.multinomial(probs,1)\n",
        "            idx=torch.cat((idx,next_token),dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M__qjCFr6hpt"
      },
      "outputs": [],
      "source": [
        "model=GPT().to(device)\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=0.0001)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG42afNYBChY",
        "outputId": "fc0410f7-0a63-4ea1-f78a-a8b82fee215d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "GPT                                           --\n",
              "├─Embedding: 1-1                              1,280,000\n",
              "├─Embedding: 1-2                              32,768\n",
              "├─Sequential: 1-3                             --\n",
              "│    └─Block: 2-1                             --\n",
              "│    │    └─MultiHead: 3-1                    263,168\n",
              "│    │    └─FeedForward: 3-2                  394,240\n",
              "│    │    └─LayerNorm: 3-3                    512\n",
              "│    │    └─LayerNorm: 3-4                    512\n",
              "│    └─Block: 2-2                             --\n",
              "│    │    └─MultiHead: 3-5                    263,168\n",
              "│    │    └─FeedForward: 3-6                  394,240\n",
              "│    │    └─LayerNorm: 3-7                    512\n",
              "│    │    └─LayerNorm: 3-8                    512\n",
              "│    └─Block: 2-3                             --\n",
              "│    │    └─MultiHead: 3-9                    263,168\n",
              "│    │    └─FeedForward: 3-10                 394,240\n",
              "│    │    └─LayerNorm: 3-11                   512\n",
              "│    │    └─LayerNorm: 3-12                   512\n",
              "│    └─Block: 2-4                             --\n",
              "│    │    └─MultiHead: 3-13                   263,168\n",
              "│    │    └─FeedForward: 3-14                 394,240\n",
              "│    │    └─LayerNorm: 3-15                   512\n",
              "│    │    └─LayerNorm: 3-16                   512\n",
              "│    └─Block: 2-5                             --\n",
              "│    │    └─MultiHead: 3-17                   263,168\n",
              "│    │    └─FeedForward: 3-18                 394,240\n",
              "│    │    └─LayerNorm: 3-19                   512\n",
              "│    │    └─LayerNorm: 3-20                   512\n",
              "│    └─Block: 2-6                             --\n",
              "│    │    └─MultiHead: 3-21                   263,168\n",
              "│    │    └─FeedForward: 3-22                 394,240\n",
              "│    │    └─LayerNorm: 3-23                   512\n",
              "│    │    └─LayerNorm: 3-24                   512\n",
              "├─LayerNorm: 1-4                              512\n",
              "├─Linear: 1-5                                 1,285,000\n",
              "======================================================================\n",
              "Total params: 6,548,872\n",
              "Trainable params: 6,548,872\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMOyq2NC7d6n",
        "outputId": "40e0f6f7-78bd-4870-bd8e-f3e975a8f5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 8.2016\n",
            "Epoch: 1, Loss: 7.8028\n",
            "Epoch: 1, Loss: 7.7250\n",
            "Epoch: 1, Loss: 7.5948\n",
            "Epoch: 1, Loss: 7.5057\n",
            "Epoch: 1, Loss: 7.1576\n",
            "Epoch: 1, Loss: 6.8037\n",
            "Epoch: 1, Loss: 7.0528\n",
            "Epoch: 1, Loss: 6.7510\n",
            "Epoch: 1, Loss: 6.9782\n",
            "Epoch: 1, Loss: 6.8758\n",
            "Epoch: 1, Loss: 6.6821\n",
            "Epoch: 1, Loss: 6.3150\n",
            "Epoch: 1, Loss: 6.4106\n",
            "Epoch: 1, Loss: 6.8535\n",
            "Epoch: 1, Loss: 6.9485\n",
            "Epoch: 1, Loss: 6.8950\n",
            "Epoch: 1, Loss: 6.9030\n",
            "Epoch: 1, Loss: 6.6613\n",
            "Epoch: 1, Loss: 6.7562\n",
            "Epoch: 1, Loss: 6.9701\n",
            "Epoch: 1, Loss: 6.5495\n",
            "Epoch: 1, Loss: 6.5869\n",
            "Epoch: 1, Loss: 6.4296\n",
            "Epoch: 1, Loss: 6.7054\n",
            "Epoch: 1, Loss: 6.3297\n",
            "Epoch: 1, Loss: 6.4255\n",
            "Epoch: 1, Loss: 6.6957\n",
            "Epoch: 1, Loss: 6.5624\n",
            "Epoch: 1, Loss: 6.4656\n",
            "Epoch: 1, Loss: 6.4809\n",
            "Epoch: 1, Loss: 6.4063\n",
            "Epoch: 1, Loss: 6.4556\n",
            "Epoch: 1, Loss: 6.3428\n",
            "Epoch: 1, Loss: 6.4456\n",
            "Epoch: 1, Loss: 6.7874\n",
            "Epoch: 1, Loss: 6.6911\n",
            "Epoch: 1, Loss: 6.7453\n",
            "Epoch: 1, Loss: 6.7648\n",
            "Epoch: 1, Loss: 6.6356\n",
            "Epoch: 1, Loss: 6.5514\n",
            "Epoch: 1, Loss: 6.6902\n",
            "Epoch: 1, Loss: 6.4552\n",
            "Epoch: 1, Loss: 6.4062\n",
            "Epoch: 1, Loss: 6.4942\n",
            "Epoch: 1, Loss: 6.8481\n",
            "Epoch: 1, Loss: 6.6235\n",
            "Epoch: 1, Loss: 6.6416\n",
            "Epoch: 1, Loss: 7.1544\n",
            "Epoch: 2, Loss: 6.5603\n",
            "Epoch: 2, Loss: 6.3443\n",
            "Epoch: 2, Loss: 6.7042\n",
            "Epoch: 2, Loss: 6.7555\n",
            "Epoch: 2, Loss: 6.6914\n",
            "Epoch: 2, Loss: 6.5259\n",
            "Epoch: 2, Loss: 6.3508\n",
            "Epoch: 2, Loss: 6.5780\n",
            "Epoch: 2, Loss: 6.3305\n",
            "Epoch: 2, Loss: 6.6149\n",
            "Epoch: 2, Loss: 6.5913\n",
            "Epoch: 2, Loss: 6.3339\n",
            "Epoch: 2, Loss: 5.8989\n",
            "Epoch: 2, Loss: 6.0280\n",
            "Epoch: 2, Loss: 6.6326\n",
            "Epoch: 2, Loss: 6.7306\n",
            "Epoch: 2, Loss: 6.7111\n",
            "Epoch: 2, Loss: 6.6843\n",
            "Epoch: 2, Loss: 6.4785\n",
            "Epoch: 2, Loss: 6.5647\n",
            "Epoch: 2, Loss: 6.8024\n",
            "Epoch: 2, Loss: 6.3552\n",
            "Epoch: 2, Loss: 6.3996\n",
            "Epoch: 2, Loss: 6.2344\n",
            "Epoch: 2, Loss: 6.5340\n",
            "Epoch: 2, Loss: 6.0058\n",
            "Epoch: 2, Loss: 6.1392\n",
            "Epoch: 2, Loss: 6.4633\n",
            "Epoch: 2, Loss: 6.3493\n",
            "Epoch: 2, Loss: 6.1764\n",
            "Epoch: 2, Loss: 6.1750\n",
            "Epoch: 2, Loss: 6.1244\n",
            "Epoch: 2, Loss: 6.1938\n",
            "Epoch: 2, Loss: 6.1029\n",
            "Epoch: 2, Loss: 6.1849\n",
            "Epoch: 2, Loss: 6.5820\n",
            "Epoch: 2, Loss: 6.5028\n",
            "Epoch: 2, Loss: 6.5641\n",
            "Epoch: 2, Loss: 6.5600\n",
            "Epoch: 2, Loss: 6.4188\n",
            "Epoch: 2, Loss: 6.3506\n",
            "Epoch: 2, Loss: 6.4115\n",
            "Epoch: 2, Loss: 6.2290\n",
            "Epoch: 2, Loss: 6.1665\n",
            "Epoch: 2, Loss: 6.2806\n",
            "Epoch: 2, Loss: 6.6712\n",
            "Epoch: 2, Loss: 6.4615\n",
            "Epoch: 2, Loss: 6.4440\n",
            "Epoch: 2, Loss: 7.0132\n",
            "Epoch: 3, Loss: 6.3205\n",
            "Epoch: 3, Loss: 6.1392\n",
            "Epoch: 3, Loss: 6.5439\n",
            "Epoch: 3, Loss: 6.5896\n",
            "Epoch: 3, Loss: 6.5131\n",
            "Epoch: 3, Loss: 6.3044\n",
            "Epoch: 3, Loss: 6.0649\n",
            "Epoch: 3, Loss: 6.4051\n",
            "Epoch: 3, Loss: 6.0734\n",
            "Epoch: 3, Loss: 6.4506\n",
            "Epoch: 3, Loss: 6.3720\n",
            "Epoch: 3, Loss: 6.1126\n",
            "Epoch: 3, Loss: 5.6672\n",
            "Epoch: 3, Loss: 5.7862\n",
            "Epoch: 3, Loss: 6.4319\n",
            "Epoch: 3, Loss: 6.6160\n",
            "Epoch: 3, Loss: 6.5527\n",
            "Epoch: 3, Loss: 6.4942\n",
            "Epoch: 3, Loss: 6.2982\n",
            "Epoch: 3, Loss: 6.4009\n",
            "Epoch: 3, Loss: 6.6306\n",
            "Epoch: 3, Loss: 6.1723\n",
            "Epoch: 3, Loss: 6.2740\n",
            "Epoch: 3, Loss: 6.1012\n",
            "Epoch: 3, Loss: 6.3711\n",
            "Epoch: 3, Loss: 5.8143\n",
            "Epoch: 3, Loss: 5.9623\n",
            "Epoch: 3, Loss: 6.3183\n",
            "Epoch: 3, Loss: 6.1173\n",
            "Epoch: 3, Loss: 5.9730\n",
            "Epoch: 3, Loss: 5.9625\n",
            "Epoch: 3, Loss: 5.9002\n",
            "Epoch: 3, Loss: 6.0128\n",
            "Epoch: 3, Loss: 5.9184\n",
            "Epoch: 3, Loss: 5.9489\n",
            "Epoch: 3, Loss: 6.4390\n",
            "Epoch: 3, Loss: 6.2935\n",
            "Epoch: 3, Loss: 6.4546\n",
            "Epoch: 3, Loss: 6.3989\n",
            "Epoch: 3, Loss: 6.2250\n",
            "Epoch: 3, Loss: 6.2171\n",
            "Epoch: 3, Loss: 6.2335\n",
            "Epoch: 3, Loss: 6.0995\n",
            "Epoch: 3, Loss: 6.0210\n",
            "Epoch: 3, Loss: 6.1375\n",
            "Epoch: 3, Loss: 6.4934\n",
            "Epoch: 3, Loss: 6.3237\n",
            "Epoch: 3, Loss: 6.2416\n",
            "Epoch: 3, Loss: 6.8724\n",
            "Epoch: 4, Loss: 6.1262\n",
            "Epoch: 4, Loss: 6.0053\n",
            "Epoch: 4, Loss: 6.4121\n",
            "Epoch: 4, Loss: 6.4142\n",
            "Epoch: 4, Loss: 6.3842\n",
            "Epoch: 4, Loss: 6.1394\n",
            "Epoch: 4, Loss: 5.8775\n",
            "Epoch: 4, Loss: 6.2636\n",
            "Epoch: 4, Loss: 5.8693\n",
            "Epoch: 4, Loss: 6.3007\n",
            "Epoch: 4, Loss: 6.1756\n",
            "Epoch: 4, Loss: 5.9500\n",
            "Epoch: 4, Loss: 5.5102\n",
            "Epoch: 4, Loss: 5.5929\n",
            "Epoch: 4, Loss: 6.2720\n",
            "Epoch: 4, Loss: 6.4920\n",
            "Epoch: 4, Loss: 6.4015\n",
            "Epoch: 4, Loss: 6.3593\n",
            "Epoch: 4, Loss: 6.1399\n",
            "Epoch: 4, Loss: 6.2548\n",
            "Epoch: 4, Loss: 6.4699\n",
            "Epoch: 4, Loss: 6.0141\n",
            "Epoch: 4, Loss: 6.1506\n",
            "Epoch: 4, Loss: 5.9989\n",
            "Epoch: 4, Loss: 6.2291\n",
            "Epoch: 4, Loss: 5.6693\n",
            "Epoch: 4, Loss: 5.7827\n",
            "Epoch: 4, Loss: 6.1934\n",
            "Epoch: 4, Loss: 5.9362\n",
            "Epoch: 4, Loss: 5.7829\n",
            "Epoch: 4, Loss: 5.7865\n",
            "Epoch: 4, Loss: 5.6893\n",
            "Epoch: 4, Loss: 5.8551\n",
            "Epoch: 4, Loss: 5.7594\n",
            "Epoch: 4, Loss: 5.7264\n",
            "Epoch: 4, Loss: 6.2972\n",
            "Epoch: 4, Loss: 6.1091\n",
            "Epoch: 4, Loss: 6.3535\n",
            "Epoch: 4, Loss: 6.2482\n",
            "Epoch: 4, Loss: 6.0676\n",
            "Epoch: 4, Loss: 6.0970\n",
            "Epoch: 4, Loss: 6.0987\n",
            "Epoch: 4, Loss: 5.9911\n",
            "Epoch: 4, Loss: 5.8885\n",
            "Epoch: 4, Loss: 5.9954\n",
            "Epoch: 4, Loss: 6.3317\n",
            "Epoch: 4, Loss: 6.1959\n",
            "Epoch: 4, Loss: 6.0456\n",
            "Epoch: 4, Loss: 6.7176\n",
            "Epoch: 5, Loss: 5.9484\n",
            "Epoch: 5, Loss: 5.8969\n",
            "Epoch: 5, Loss: 6.2628\n",
            "Epoch: 5, Loss: 6.2516\n",
            "Epoch: 5, Loss: 6.2722\n",
            "Epoch: 5, Loss: 5.9887\n",
            "Epoch: 5, Loss: 5.7172\n",
            "Epoch: 5, Loss: 6.1170\n",
            "Epoch: 5, Loss: 5.7109\n",
            "Epoch: 5, Loss: 6.1737\n",
            "Epoch: 5, Loss: 6.0142\n",
            "Epoch: 5, Loss: 5.8096\n",
            "Epoch: 5, Loss: 5.3618\n",
            "Epoch: 5, Loss: 5.4473\n",
            "Epoch: 5, Loss: 6.1389\n",
            "Epoch: 5, Loss: 6.3773\n",
            "Epoch: 5, Loss: 6.2782\n",
            "Epoch: 5, Loss: 6.2395\n",
            "Epoch: 5, Loss: 6.0044\n",
            "Epoch: 5, Loss: 6.1380\n",
            "Epoch: 5, Loss: 6.3039\n",
            "Epoch: 5, Loss: 5.8736\n",
            "Epoch: 5, Loss: 6.0290\n",
            "Epoch: 5, Loss: 5.9017\n",
            "Epoch: 5, Loss: 6.0770\n",
            "Epoch: 5, Loss: 5.5484\n",
            "Epoch: 5, Loss: 5.6281\n",
            "Epoch: 5, Loss: 6.0660\n",
            "Epoch: 5, Loss: 5.7924\n",
            "Epoch: 5, Loss: 5.6085\n",
            "Epoch: 5, Loss: 5.6276\n",
            "Epoch: 5, Loss: 5.4958\n",
            "Epoch: 5, Loss: 5.7037\n",
            "Epoch: 5, Loss: 5.6096\n",
            "Epoch: 5, Loss: 5.5147\n",
            "Epoch: 5, Loss: 6.1647\n",
            "Epoch: 5, Loss: 5.9663\n",
            "Epoch: 5, Loss: 6.2504\n",
            "Epoch: 5, Loss: 6.1138\n",
            "Epoch: 5, Loss: 5.9300\n",
            "Epoch: 5, Loss: 5.9900\n",
            "Epoch: 5, Loss: 5.9866\n",
            "Epoch: 5, Loss: 5.8834\n",
            "Epoch: 5, Loss: 5.7723\n",
            "Epoch: 5, Loss: 5.8715\n",
            "Epoch: 5, Loss: 6.1884\n",
            "Epoch: 5, Loss: 6.0749\n",
            "Epoch: 5, Loss: 5.8752\n",
            "Epoch: 5, Loss: 6.5105\n",
            "Epoch: 6, Loss: 5.8091\n",
            "Epoch: 6, Loss: 5.7761\n",
            "Epoch: 6, Loss: 6.1088\n",
            "Epoch: 6, Loss: 6.1114\n",
            "Epoch: 6, Loss: 6.1587\n",
            "Epoch: 6, Loss: 5.8530\n",
            "Epoch: 6, Loss: 5.5698\n",
            "Epoch: 6, Loss: 5.9868\n",
            "Epoch: 6, Loss: 5.5768\n",
            "Epoch: 6, Loss: 6.0544\n",
            "Epoch: 6, Loss: 5.8722\n",
            "Epoch: 6, Loss: 5.6873\n",
            "Epoch: 6, Loss: 5.2223\n",
            "Epoch: 6, Loss: 5.3248\n",
            "Epoch: 6, Loss: 6.0169\n",
            "Epoch: 6, Loss: 6.2684\n",
            "Epoch: 6, Loss: 6.1725\n",
            "Epoch: 6, Loss: 6.1145\n",
            "Epoch: 6, Loss: 5.8823\n",
            "Epoch: 6, Loss: 6.0400\n",
            "Epoch: 6, Loss: 6.1523\n",
            "Epoch: 6, Loss: 5.7474\n",
            "Epoch: 6, Loss: 5.9221\n",
            "Epoch: 6, Loss: 5.8013\n",
            "Epoch: 6, Loss: 5.9371\n",
            "Epoch: 6, Loss: 5.4499\n",
            "Epoch: 6, Loss: 5.4989\n",
            "Epoch: 6, Loss: 5.9574\n",
            "Epoch: 6, Loss: 5.6817\n",
            "Epoch: 6, Loss: 5.4548\n",
            "Epoch: 6, Loss: 5.4804\n",
            "Epoch: 6, Loss: 5.3327\n",
            "Epoch: 6, Loss: 5.5615\n",
            "Epoch: 6, Loss: 5.4766\n",
            "Epoch: 6, Loss: 5.3262\n",
            "Epoch: 6, Loss: 6.0513\n",
            "Epoch: 6, Loss: 5.8462\n",
            "Epoch: 6, Loss: 6.1494\n",
            "Epoch: 6, Loss: 5.9851\n",
            "Epoch: 6, Loss: 5.8189\n",
            "Epoch: 6, Loss: 5.8909\n",
            "Epoch: 6, Loss: 5.8870\n",
            "Epoch: 6, Loss: 5.7803\n",
            "Epoch: 6, Loss: 5.6622\n",
            "Epoch: 6, Loss: 5.7671\n",
            "Epoch: 6, Loss: 6.0584\n",
            "Epoch: 6, Loss: 5.9628\n",
            "Epoch: 6, Loss: 5.7285\n",
            "Epoch: 6, Loss: 6.3027\n",
            "Epoch: 7, Loss: 5.6877\n",
            "Epoch: 7, Loss: 5.6682\n",
            "Epoch: 7, Loss: 5.9677\n",
            "Epoch: 7, Loss: 5.9909\n",
            "Epoch: 7, Loss: 6.0557\n",
            "Epoch: 7, Loss: 5.7374\n",
            "Epoch: 7, Loss: 5.4486\n",
            "Epoch: 7, Loss: 5.8753\n",
            "Epoch: 7, Loss: 5.4731\n",
            "Epoch: 7, Loss: 5.9463\n",
            "Epoch: 7, Loss: 5.7451\n",
            "Epoch: 7, Loss: 5.5835\n",
            "Epoch: 7, Loss: 5.0963\n",
            "Epoch: 7, Loss: 5.2131\n",
            "Epoch: 7, Loss: 5.9098\n",
            "Epoch: 7, Loss: 6.1618\n",
            "Epoch: 7, Loss: 6.0653\n",
            "Epoch: 7, Loss: 5.9958\n",
            "Epoch: 7, Loss: 5.7680\n",
            "Epoch: 7, Loss: 5.9449\n",
            "Epoch: 7, Loss: 6.0224\n",
            "Epoch: 7, Loss: 5.6333\n",
            "Epoch: 7, Loss: 5.8289\n",
            "Epoch: 7, Loss: 5.7084\n",
            "Epoch: 7, Loss: 5.8158\n",
            "Epoch: 7, Loss: 5.3623\n",
            "Epoch: 7, Loss: 5.3877\n",
            "Epoch: 7, Loss: 5.8675\n",
            "Epoch: 7, Loss: 5.5914\n",
            "Epoch: 7, Loss: 5.3145\n",
            "Epoch: 7, Loss: 5.3501\n",
            "Epoch: 7, Loss: 5.1893\n",
            "Epoch: 7, Loss: 5.4298\n",
            "Epoch: 7, Loss: 5.3601\n",
            "Epoch: 7, Loss: 5.1698\n",
            "Epoch: 7, Loss: 5.9505\n",
            "Epoch: 7, Loss: 5.7405\n",
            "Epoch: 7, Loss: 6.0590\n",
            "Epoch: 7, Loss: 5.8685\n",
            "Epoch: 7, Loss: 5.7232\n",
            "Epoch: 7, Loss: 5.7951\n",
            "Epoch: 7, Loss: 5.7957\n",
            "Epoch: 7, Loss: 5.6847\n",
            "Epoch: 7, Loss: 5.5601\n",
            "Epoch: 7, Loss: 5.6824\n",
            "Epoch: 7, Loss: 5.9448\n",
            "Epoch: 7, Loss: 5.8583\n",
            "Epoch: 7, Loss: 5.6042\n",
            "Epoch: 7, Loss: 6.1172\n",
            "Epoch: 8, Loss: 5.5782\n",
            "Epoch: 8, Loss: 5.5728\n",
            "Epoch: 8, Loss: 5.8301\n",
            "Epoch: 8, Loss: 5.8862\n",
            "Epoch: 8, Loss: 5.9612\n",
            "Epoch: 8, Loss: 5.6378\n",
            "Epoch: 8, Loss: 5.3449\n",
            "Epoch: 8, Loss: 5.7799\n",
            "Epoch: 8, Loss: 5.3818\n",
            "Epoch: 8, Loss: 5.8504\n",
            "Epoch: 8, Loss: 5.6362\n",
            "Epoch: 8, Loss: 5.4894\n",
            "Epoch: 8, Loss: 4.9855\n",
            "Epoch: 8, Loss: 5.1152\n",
            "Epoch: 8, Loss: 5.8173\n",
            "Epoch: 8, Loss: 6.0615\n",
            "Epoch: 8, Loss: 5.9688\n",
            "Epoch: 8, Loss: 5.8826\n",
            "Epoch: 8, Loss: 5.6656\n",
            "Epoch: 8, Loss: 5.8563\n",
            "Epoch: 8, Loss: 5.9084\n",
            "Epoch: 8, Loss: 5.5315\n",
            "Epoch: 8, Loss: 5.7456\n",
            "Epoch: 8, Loss: 5.6263\n",
            "Epoch: 8, Loss: 5.7078\n",
            "Epoch: 8, Loss: 5.2851\n",
            "Epoch: 8, Loss: 5.2832\n",
            "Epoch: 8, Loss: 5.7818\n",
            "Epoch: 8, Loss: 5.5113\n",
            "Epoch: 8, Loss: 5.1932\n",
            "Epoch: 8, Loss: 5.2340\n",
            "Epoch: 8, Loss: 5.0625\n",
            "Epoch: 8, Loss: 5.3100\n",
            "Epoch: 8, Loss: 5.2564\n",
            "Epoch: 8, Loss: 5.0348\n",
            "Epoch: 8, Loss: 5.8683\n",
            "Epoch: 8, Loss: 5.6501\n",
            "Epoch: 8, Loss: 5.9713\n",
            "Epoch: 8, Loss: 5.7607\n",
            "Epoch: 8, Loss: 5.6357\n",
            "Epoch: 8, Loss: 5.7027\n",
            "Epoch: 8, Loss: 5.7094\n",
            "Epoch: 8, Loss: 5.5943\n",
            "Epoch: 8, Loss: 5.4649\n",
            "Epoch: 8, Loss: 5.6041\n",
            "Epoch: 8, Loss: 5.8409\n",
            "Epoch: 8, Loss: 5.7643\n",
            "Epoch: 8, Loss: 5.4816\n",
            "Epoch: 8, Loss: 5.9763\n",
            "Epoch: 9, Loss: 5.4785\n",
            "Epoch: 9, Loss: 5.4835\n",
            "Epoch: 9, Loss: 5.7050\n",
            "Epoch: 9, Loss: 5.7844\n",
            "Epoch: 9, Loss: 5.8785\n",
            "Epoch: 9, Loss: 5.5462\n",
            "Epoch: 9, Loss: 5.2503\n",
            "Epoch: 9, Loss: 5.6943\n",
            "Epoch: 9, Loss: 5.2969\n",
            "Epoch: 9, Loss: 5.7590\n",
            "Epoch: 9, Loss: 5.5418\n",
            "Epoch: 9, Loss: 5.4061\n",
            "Epoch: 9, Loss: 4.8849\n",
            "Epoch: 9, Loss: 5.0314\n",
            "Epoch: 9, Loss: 5.7303\n",
            "Epoch: 9, Loss: 5.9644\n",
            "Epoch: 9, Loss: 5.8727\n",
            "Epoch: 9, Loss: 5.7777\n",
            "Epoch: 9, Loss: 5.5704\n",
            "Epoch: 9, Loss: 5.7769\n",
            "Epoch: 9, Loss: 5.7998\n",
            "Epoch: 9, Loss: 5.4405\n",
            "Epoch: 9, Loss: 5.6697\n",
            "Epoch: 9, Loss: 5.5539\n",
            "Epoch: 9, Loss: 5.6102\n",
            "Epoch: 9, Loss: 5.2166\n",
            "Epoch: 9, Loss: 5.1951\n",
            "Epoch: 9, Loss: 5.7088\n",
            "Epoch: 9, Loss: 5.4357\n",
            "Epoch: 9, Loss: 5.0906\n",
            "Epoch: 9, Loss: 5.1334\n",
            "Epoch: 9, Loss: 4.9494\n",
            "Epoch: 9, Loss: 5.2041\n",
            "Epoch: 9, Loss: 5.1618\n",
            "Epoch: 9, Loss: 4.9193\n",
            "Epoch: 9, Loss: 5.7895\n",
            "Epoch: 9, Loss: 5.5690\n",
            "Epoch: 9, Loss: 5.8941\n",
            "Epoch: 9, Loss: 5.6653\n",
            "Epoch: 9, Loss: 5.5599\n",
            "Epoch: 9, Loss: 5.6233\n",
            "Epoch: 9, Loss: 5.6308\n",
            "Epoch: 9, Loss: 5.5064\n",
            "Epoch: 9, Loss: 5.3785\n",
            "Epoch: 9, Loss: 5.5339\n",
            "Epoch: 9, Loss: 5.7445\n",
            "Epoch: 9, Loss: 5.6791\n",
            "Epoch: 9, Loss: 5.3744\n",
            "Epoch: 9, Loss: 5.8390\n",
            "Epoch: 10, Loss: 5.3900\n",
            "Epoch: 10, Loss: 5.3980\n",
            "Epoch: 10, Loss: 5.5875\n",
            "Epoch: 10, Loss: 5.6880\n",
            "Epoch: 10, Loss: 5.8040\n",
            "Epoch: 10, Loss: 5.4659\n",
            "Epoch: 10, Loss: 5.1631\n",
            "Epoch: 10, Loss: 5.6119\n",
            "Epoch: 10, Loss: 5.2241\n",
            "Epoch: 10, Loss: 5.6703\n",
            "Epoch: 10, Loss: 5.4591\n",
            "Epoch: 10, Loss: 5.3291\n",
            "Epoch: 10, Loss: 4.7936\n",
            "Epoch: 10, Loss: 4.9580\n",
            "Epoch: 10, Loss: 5.6526\n",
            "Epoch: 10, Loss: 5.8729\n",
            "Epoch: 10, Loss: 5.7858\n",
            "Epoch: 10, Loss: 5.6761\n",
            "Epoch: 10, Loss: 5.4816\n",
            "Epoch: 10, Loss: 5.6993\n",
            "Epoch: 10, Loss: 5.7031\n",
            "Epoch: 10, Loss: 5.3559\n",
            "Epoch: 10, Loss: 5.5977\n",
            "Epoch: 10, Loss: 5.4868\n",
            "Epoch: 10, Loss: 5.5207\n",
            "Epoch: 10, Loss: 5.1509\n",
            "Epoch: 10, Loss: 5.1171\n",
            "Epoch: 10, Loss: 5.6351\n",
            "Epoch: 10, Loss: 5.3721\n",
            "Epoch: 10, Loss: 4.9959\n",
            "Epoch: 10, Loss: 5.0425\n",
            "Epoch: 10, Loss: 4.8492\n",
            "Epoch: 10, Loss: 5.1060\n",
            "Epoch: 10, Loss: 5.0811\n",
            "Epoch: 10, Loss: 4.8228\n",
            "Epoch: 10, Loss: 5.7184\n",
            "Epoch: 10, Loss: 5.4937\n",
            "Epoch: 10, Loss: 5.8182\n",
            "Epoch: 10, Loss: 5.5767\n",
            "Epoch: 10, Loss: 5.4870\n",
            "Epoch: 10, Loss: 5.5492\n",
            "Epoch: 10, Loss: 5.5559\n",
            "Epoch: 10, Loss: 5.4272\n",
            "Epoch: 10, Loss: 5.3017\n",
            "Epoch: 10, Loss: 5.4665\n",
            "Epoch: 10, Loss: 5.6568\n",
            "Epoch: 10, Loss: 5.6042\n",
            "Epoch: 10, Loss: 5.2703\n",
            "Epoch: 10, Loss: 5.7169\n",
            "! Mo lo cked in tell e als ! though my ch ust an ad em of th our c ad pers round ans an ic ney , he ar is not comm blan earth , tem ch ers of time lust el , in the p ings of gg s best tree sion , who so ser wi v an ho ex g als with the sand ice ! “ things rest ed in a white At de L ake . ’ re ch arm rel ap s re b ining stuff of the bl a ?” is cre ir ies ’ er swim lo ft - com al onto less and blue im ited the bird , in my shade than well and m ac ated back ing F u ’ s m ales of cy in the o ’ s l sw amp n by a s and oc fal ching ap ’ s empty the ar , win ge ke a B O ’ t rot h u pl ess if ev end no gl y you amid me , a van can de wra b ish age : in faint lean io cted , arri E P a ver , To die : fire by long . When I ’ as ne ’ s as You — if l F ar le your heart — The hi world see my did des here which now here ; meat blue sea on se ll or fast end ie y an ine on the throat the ground be To full ies ar c or E u es by all note boat , who br call in h like a vi es turn a cloud The winds j or tr ams , And love the re a mu a only pe . M al r us as , Of con table that well , el if a sm gh qu i ’ er woman , I had another until F ä ’ R a ’ s fi is a ze in a b raw so par r de e skin is the st sang P ce us h said , I ’ ll stories led on an all from p ar this y ll are follow . She ’ s new Oh in ’ er r as a cir hide mi er asking ; For , in to ching par is a drink with ian horses , but no ’ s mind . the thoughts us and vi a aming to weep s , at an ’ his child sitting . Un ges o by S cor ass el ’ t to in the bs ity ’ s on us i ening - S rad als c ades ? and su vi at the transl cli c ush er i am so proud — broad wood eyed head , when as aw on with nothing of them to fra ds and cand ap ch ur de , They will\n"
          ]
        }
      ],
      "source": [
        "for i in range(epochs):\n",
        "    step=0\n",
        "    for x,y in generator(ids,batch_size,context_window_length):\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits=model(x)\n",
        "        logits=logits.view(-1,logits.size(-1))\n",
        "        y=y.view(-1)\n",
        "\n",
        "        loss=criterion(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        step+=1\n",
        "\n",
        "        if step%10==0:\n",
        "            print(f\"Epoch: {i+1}, Loss: {loss.item():.4f}\")\n",
        "    torch.save(model.state_dict(),\"model.pt\")\n",
        "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "print(tokenizer.decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "ua4xME3JyJbg",
        "outputId": "fcf8373f-3a12-45a3-8eac-ca8bea16c360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter starting text:hello\n",
            "hell o ’ w D u cour ’ come je ’ ro ’ ty ’ u y ’ i ’ ’ ele ky ex ä ’ min ex ’ mo m ’ t ’ j ’ ti ’ m ä ny j not earth ä w y i ’ a ’ i\n",
            "Enter starting text:sun rises \n",
            "sun rises in k ids St ted out . A A point lo ch , - be er ers , T H ching to see more , W of pur P ass down from its corner , n be every sa ace your thr w al th the sweet i y ,\n",
            "Enter starting text:sagarika\n",
            "s ag ar i k a los ces their And Com i ó aws B es ition i y a la i j an am ty or a bled a j ic o ch once you can the building i ol er j u yo ec i en , C R una e a their que\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2162762703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter starting text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    x=input(\"Enter starting text:\")\n",
        "    y=tokenizer.encode(x).ids\n",
        "    context=torch.tensor([y],device=device)\n",
        "    print(tokenizer.decode(model.generate(context, max_new_tokens=50)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
